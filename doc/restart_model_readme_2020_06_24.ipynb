{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "restart-model-readme-2020-06-24.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "MsdJwfq15v33",
    "H1uq0RMITXy3",
    "MDnksm1MTmV7",
    "38KeztwBajb8",
    "gXxIvzREazRq",
    "7AulXwNtb5yd",
    "GDR12BB4pPd1"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "",
   "display_name": ""
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/restartus/covid-projection/blob/rich-demo/model/restart_model_readme_2020_06_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPP19bJbIKev",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "_Proprietary and Confidential. Do not distribute without permission._\n",
    "\n",
    "---\n",
    "\n",
    "# Restart Partners   - Sample Memo\n",
    "\n",
    "---\n",
    "_To:_ Jun Amora (Mayors Office, City of New York)  \n",
    "_From:_ Bharat Shyam, Rich Tong (Restart)  \n",
    "_Re:_ Analysis for NYC PPE needs  \n",
    "_Date:_ 20 May 2020  \n",
    "\n",
    "--- \n",
    "\n",
    "New York City needs a 90-day stockpile for the heathcare workers, first responders and congregate care facilities is really important, but coming up with an estimate for this is difficult given the variability of the infection and the uncertainty in the degree of economic recovery and social mobility. Therefore, we are providing another resource model to augment yours that shows that our figures are within 30%-50% of your bottoms estimate. Given that we are happy to:\n",
    "\n",
    "- _Refine healthcare estimates_. All models are heavily dependent on estimates of population involved and usage data. \n",
    "- _Non-healthcare estimates_. For instance, this model does project needs outside of the healthcare area such as small business, vertical industries and vulnerable populations.\n",
    "- _Long-term modeling_. We are extending the model to include test equipment, disinfectant wipes and liquid disinfectants, so happy to add things that you need. Also we will be integrating epi and economic models too and would love to partner with you on that.\n",
    "\n",
    "Given the uncertainties involved, this might help you make the right estimates. What follows next are:\n",
    "\n",
    "1. Disclaimer. This is not a definitive estimate. You should use other sources and information to make your decisions.\n",
    "2. Data Sources. We have included the model source data, how the model is constructed and then results. Feel free to use this data and modify as appropriate, but it serves as documentation for all the assumptions made.\n",
    "3. Model. The way the calculation is done with assumptions and resulting projections\n",
    "4. Outputs. The conclusions we can draw from the projection.\n",
    "\n",
    "## Disclaimer\n",
    "It must be noted that the Restart Partners (\"Restart\") Equipment Model (the \"Model\") is made available for public use free of charge. Determining equipment needs for each jurisdiction, entity or other party (each a \"User\") is a complex and multifaceted decision process. Restart does not does not have the authority or ability to assign empirical risks levels nor make definitive use decisions for any User. Rather, the Model provides one approach to making recommendations that can help Users make decisions about their potential equipment uses by allowing them to calculate their potential requirements. Users are strongly encouraged to consider other sources of information and expressly disclaim any cause of action they may have against Restart arising from or relating to the Model or its analysis. Implementing the equipment levels projected by the Model will not eliminate the risk of COID-19 cases being linked to activites in an economy or workplace. In this context, it is important to note that this equipment alone will not eliminate the risk of infection. All Users should remain informed about and abide by any decisions made by local public health and government authorities regarding specific mitigation efforts, including equipment in the model, as the situation is dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xo1YI-GvU4o",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "# Model Data for New York use Washington Consumption\n",
    "\n",
    "Because we do not have New York City specific data, we used various open data sources to fill in the five major assumptions in the model:\n",
    "\n",
    "1. Usage by Population. This cuts the item usage per person per day. This right now is a series of levels. So we have four levels for civilians and then two levels for healthcare workers.\n",
    "2. Usage per Patient. This is the way Epidemiological models work. That is, given a number of patients, calculate how much they will need. The model currently uses the [WHO Surge Essential Supplies Forecasting Tool v1.2](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/covid-19-critical-items) and estimates the entire US population use with 1,000 cases and fast transmission and slow response. So this is a very pessimistic scenario. This makes sense when calculating the surge estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOPqqurOE8-E",
    "colab_type": "text"
   },
   "source": [
    "# Follow-on versions from v2 to v2.x\n",
    "\n",
    "The next steps are a little complicated, but we are converting the entire model into a series of vectorized operations. This documents:\n",
    "\n",
    "## v2. Duplicate the Excel Surge\n",
    "\n",
    "This duplicates the current surge model that is run by the Excel sheets as of v1.4.7. This is meant as a check of formulas and to ensure have duplicated the model.\n",
    "\n",
    "\n",
    "## v2.1 Add patients, more tests and tempos\n",
    "\n",
    "This includes calculations for adding patients counts and test tempos. This assumes that testing is either brought in as a separate module and the patients as well as a function of either SIR data or with some test method such as used by BMGF spreadsheets or WHO. It also adds burn rates by more than just daily per capita.\n",
    "\n",
    "## v2.2. Add ranges of costs\n",
    "\n",
    "In parenthesis are the notes for integrating the new features of v2.x. This includes a time series as the main features\n",
    "\n",
    "## v2.3 Add Time series\n",
    "\n",
    "So we do more than the surge model\n",
    "\n",
    "## v2.4 Think aboiut doing distributions rather than just ranges\n",
    "\n",
    "Would have a range of units increasing the dimensionality again. At this point, we might move to a mean with a sigma instead of many different parameters. This would mean we would have to do distribution math which asusming we have normal distributions is possible. For instance [normal distribution math](https://math.stackexchange.com/questions/2314763/variance-operations-on-the-normal-distribution) shows in [Latex](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)\n",
    "\n",
    "$$\n",
    "X = N(\\mu, \\sigma^2)\\\\\n",
    "Y = N(\\upsilon, \\tau^2)\\\\\n",
    "aX + bY = N(a\\mu + b\\upsilon, a^2\\sigma^2 +b^2\\tau^2)\n",
    "$$\n",
    "\n",
    "And I'm pretty sure this distribution math is a numpy function somewhere :-). However, the [product of two normal distributions](http://pldml.icm.edu.pl/pldml/element/bwmeta1.element.bwnjournal-article-doi-10_7151_dmps_1146/c/dmps.1146.pdf) is not normal, it is a Bessel function so it might be better just to use ranges which are a little easier to manage. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ3osyCnkLWQ",
    "colab_type": "text"
   },
   "source": [
    "# A note on notation\n",
    "We have two sets of notation:\n",
    "\n",
    "- Formulaic notation using Latex that is suitable for math and proofs. It more concise, but derivable from and convertable to the long code names.\n",
    "- Notation for coding in Python. In this we are using some coding conventions as the terms can be confusing. We use snake_case for local variable and suffix each matrix or array with their dimensions. We use unique letters for various dimensions. For instance, we have p populations and n resources, so a matrix which is p rows and n columns would look like `data_pn`\n",
    "\n",
    "## Einstein Summation\n",
    "Besides matrix multiplication and element-wise multiplication, we also make sure of [Einstein Summation](https://mathworld.wolfram.com/EinsteinSummation.html) popularized by Einstein's use in proving his theories, the notation allows axis summation, so that each index indicates what is iterated over and the left hand indicates what is summed or squashed\n",
    "\n",
    "$$\n",
    "P^T_{pn} = P_{pn}\\,PE^T_{tnp}\n",
    "$$\n",
    "\n",
    "This is exactly equivalent to \n",
    "\n",
    "    PT_tpn = np.einsum(\"dpn,np=pn\",P_dpn, T_np)\n",
    "\n",
    "## The use of suffixes and fixed constants for array dimensions\n",
    "- Also for some of these the type is not obvious, so for simplicity there is another suffice for the type where `df` means Pandas DataFrame and `arr` means Numpy Array.\n",
    "- Then we try to be descriptive of the actual variable itself so for instance if we are dealing with consumption or burn rates, then it would be `consumption_pn` means what is the consumption for each population p for every n resource. The longer hand would be `pop_consumption_pn_df` so there are some consistent abbreviations\n",
    "- The use of array variable makes the use of Einstein Summations much clearer, instead of the normal `ij,jk=ik`, it is way more understandable to write `pn,nk=pk` which would be p population rows by k variables\n",
    "\n",
    "## The main class of objects in the model and their relationship to notation\n",
    "\n",
    "The general scheme is that there is a class Model which gets instantiated with all the appropriate labels. This provides the implicit dimenstions of the model.\n",
    "\n",
    "Then each major module can be subclassed from this and your can replace it. The current list of modules are and in the notation, the class name is the major part of the variable\n",
    "\n",
    "- Model. This is the core framework. It holds the dimensions and pointers to all the module component instances.\n",
    "\n",
    "The objects in the world which has a single character $symbol$ or a 3-4 character `_short_name_` name\n",
    "- Population as $P$ or `pop`. This holds the population and details about it \n",
    "- Resource $R$ or `res`. The resources needed\n",
    "- Essentiality $E$ or `ess`. The population summarized by summary levels that are used to restart the economy in phases and stop it the same way\n",
    "\n",
    "How the resources are stored and used:\n",
    "- Demand $D$ or `demand`. This calculates the burn rates or usage of the products. In the surge model these are done per person per day which generates an lxn matrix\n",
    "- Supply $S$ or `supp`. The sources of supply and input of resources\n",
    "- Inventory $I$ or `inv`. What is currenty on hand\n",
    "\n",
    "Finally there are various properties that these objects can have. these are handles as superscripts in the formula notation or as the second word in the code as snake_case.\n",
    "\n",
    "- Cost $C$ or `cost`. The cost per unit\n",
    "- Total $T$ or `total_cost`. The total for an entire population\n",
    "\n",
    "## The names of various arrays that are data sources\n",
    "\n",
    "In formulas, the single letter capitals are data while if there are two letters, it indicates a transformation, so the basic data is as single letter denoting the header and then a subscript on what is being measures. These are all attached to classes\n",
    "\n",
    "- $P_{pd}$ or `Population.pop_df`. Population data where p is the number of populations and can have d details about it like how many \"runs per day\" for an EMT or number of covid patients\n",
    "- $R_{na}$ or `Resource.res_na[n, a]` were Resource data for n items with a attributes (like it's volume and cost), by convention, the first column has a one in it for easy matrix multiplication\n",
    "- $P^R_{pn}$ or `Population.to_res_pn[p, n]`. This is a given populations use of all n resources and is the per capita, per day figure.\n",
    "- $P^T_{pn}$ or `Population.total_pn[p, n]`. This is the total population usage\n",
    "- $E^C_{en}$ or `Essential.cost_en[e, n]`. for every essentiality level, what is the cost for each resource n.\n",
    "- $E^T_{en}$ or `Essential.total_cost_en[e, n]. The total cost for every essential level for each resource\n",
    "\n",
    "## The names of array that convert from one datum to another matrix\n",
    "\n",
    "Then there are conversions between these note that these are two letter variables that denote the source and then the destination that is getting converted. In formulas, we denote the with a superscript$^t$ and with Python we use the `**from**_to_**to**` in the Snake case \n",
    "\n",
    "- $D^{Rt}_{ln}$ or `Demand.for_resource_ln`. This is the conversion from essential levels to items used where l is the number of levels and n is the number of resources. It is the core burn rate analysis\n",
    "- $D^{Ut}_{p,l}$ or `Demand.pop_to_usage_pl[p, l]`. Converts a population into levels of use\n",
    "- $P^{Et}_{p,e}$ or `Population.to_essentiality_pe[p, e]`. This is the transformation of Population to Essential levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsdJwfq15v33",
    "colab_type": "text"
   },
   "source": [
    "# The Component architecture of the code classes\n",
    "\n",
    "The architecture of system models the three things happening in the real world which are the relationships between a population at the center. Here are the basic components of the v2 model and the graphical representation is below:\n",
    "\n",
    "1. Population. This contains all the data on the people under study.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TK0tkgHU5Oaj",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "outputId": "f184224e-b665-4c51-b036-b898f0889f4f"
   },
   "source": [
    "# https://h1ros.github.io/posts/introduction-to-graphviz-in-jupyter-notebook/\n",
    "# https://www.graphviz.org/pdf/dotguide.pdf for directed graphs manual\n",
    "# https://www.graphviz.org/pdf/neatoguide.pdf for undirected manual\n",
    "# https://graphviz.readthedocs.io/en/stable/manual.html\n",
    "from graphviz import Digraph, Graph\n",
    "dot = Digraph('Class Model', node_attr={'shape': 'box'})\n",
    "neato = Graph(engine='neato')\n",
    "\n",
    "# Population superclass using cluster as a special name\n",
    "# Note that name must be a named parameter\n",
    "# https://graphviz.readthedocs.io/en/stable/examples.html#cluster-py\n",
    "with dot.subgraph(name='Pop_class') as c:\n",
    "  c.attr(style='filled')\n",
    "  c.node('P', 'Population, Essentiality')\n",
    "\n",
    "dot.node('D', 'Disease')\n",
    "dot.edge('P', 'D', 'Social Mobility')\n",
    "dot.edge('D', 'P', 'Patients')\n",
    "\n",
    "dot.node('E', 'Economy')\n",
    "dot.edge('P', 'E', 'Stage, Economic Activity')\n",
    "dot.edge('E', 'P', 'GDP, Employment')\n",
    "\n",
    "with dot.subgraph(name=\"Res\") as c:\n",
    "  c.node('R', 'Resource')\n",
    "  c.edge('R', 'P', 'Delivery')\n",
    "  c.edge('P', 'R', 'Demand' )\n",
    "  c.node('I', 'Inventory')\n",
    "  c.edge('R', 'I', 'Fill')\n",
    "  c.edge('I', 'R', 'Use')\n",
    "\n",
    "dot.node('S', 'Supply')\n",
    "dot.edge('R', 'S', 'Sales Order')\n",
    "dot.edge('S', 'R', 'Fulfillment')\n",
    "\n",
    "\n",
    "# now render\n",
    "display(dot)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aes2JqzOkk9G",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HROEvhCdl7m4",
    "colab_type": "text"
   },
   "source": [
    "# Model Class\n",
    "\n",
    "Where it all begins, this connects the various classes together and does the model logic creating new objects.\n",
    "\n",
    "v2.3. An idea is that when you change this, how does it dynamically recalculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRPqcRMfklXE",
    "colab_type": "text"
   },
   "source": [
    "# Resource Class: `Resource.res(n,1)`, $R_n$\n",
    "\n",
    "This in the simplest form is just a vector of 1s with the labels being the materials.\n",
    "\n",
    "   Resource[n, 1] = np.ones((n, 1))\n",
    "\n",
    "We can handle any arbitrary vector of items, just change the list. Longer term, we will have our own GUID system, but right no rely on unique text strings\n",
    "\n",
    "## A note on handling conservation by amortizing specific items.\n",
    "\n",
    "In the v1.x models, we had a series of calculations indicating which items were reusable and which weren't. With an unlimited number of columns, it is much eaiser to just have dedicated items for:\n",
    "\n",
    "- Cloth masks\n",
    "- Disinfectable N95 respirators\n",
    "\n",
    "Then if an article lasts n days, set their tempo of use to 1/n, so a cloth mask that can be used for 10 days for disposal would be 0.1.\n",
    "\n",
    "This does not quite model the spike and flow, but is a decent approximation for now over many days.\n",
    "\n",
    "# Resource Class: Physical facts about resources, `Resource.res[d, n]`, $R_{dn}$ for v2.2\n",
    "\n",
    "\n",
    "We need to know some physical characteristics about resources. This could also come from Supply but will likely be more static. Some of the key ones are the volume and count\n",
    "\n",
    "    Resource_dn = [ units, square footage, weight, volume \n",
    "\n",
    "The other variables for the resource are physical characteristics like the volume of the package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohnMmgU5mANo",
    "colab_type": "text"
   },
   "source": [
    "# Population Class yields `Population.pop_p[p,1]`, $P_{p}$  \n",
    "\n",
    "In the original model, we had p sub-populations. In the simplest model, it was just two populations: non-employees of healthcare companies and employees. With SOC and other codes, there are close to a thousands. So this is a column vector\n",
    "\n",
    "So for example `Population_p = [ [[7400], [435]] ]` which is just about the right numbers for Seattle and is a $[p, 1$ vector \n",
    "\n",
    "## `Population_pa` or $P_{pd}$ in v2.1 and higher\n",
    "v2.1 adds more population details d which can be for instance run rates for EMTs, number of patients so this become `population_pd` $P(p,d)$)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUrTlN99mCpL",
    "colab_type": "text"
   },
   "source": [
    "# Demand Class: `demand.use_res_ln[l,n]`, $DR^T_{ln}$\n",
    "\n",
    "So first we need the inputs which are the list of usage or protection levels l x the number of resources we are tracking n. So this is an p x n matrix where each entry says for protection level l, we have so many items per capita per day. So the first data list is `demand_to_res_ln[l, n]` and $DR^T_{ln} = l rows x n columns$ or demand_to_res.shape = ( l, n )\n",
    "\n",
    "In Python speak this is `demand_to_res_ln_df` since it is a Panda **D**ata**F**rame. Right now this is a test matrix that is constructed in the Jupyter Notebook, but longer term, it should be pulled from an database with the suitable annotations.\n",
    "\n",
    "## v2. Demand adds a detail level for multiple tempos `demand_res_dln[d, l, n]`, $DR^T_{dln}$\n",
    "v2.1 will add a population detail level so you can have tempos of things other than per capita, for instance, it could be per run or per COVID patient in the population and then it becomes `Usage_to_res_dln $C[d, n, l]` where d are the population Details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2CnVk8Zklzg",
    "colab_type": "text"
   },
   "source": [
    "# Population Class: Population spread over different Usage levels of resources: `Population.to_usage_pl`, $PU_{pl}$\n",
    "\n",
    "You can think of this as a one-hot matrix in the most simple form. That is for each population, what level of protection do you need. \n",
    "\n",
    "For example, in the simplest case, if there are six protection levels, then if non-employees get level 1 and healthcare employees get level 6, then the matrix looks so then `Pop_to_usage_pl` is:\n",
    "\n",
    "    0 1 0 0 0 0 0\n",
    "    0 0 0 0 0 0 1\n",
    "\n",
    "While date entry is complicated, this let's you take any given population and give it fractions of protection. For instance, if a healthcare employer typically had 50% of it's workers as office workers at level 2, 25% as customer facing and 10% taking care of non COVID and 15% in direct contact, that vector would look like:\n",
    "\n",
    "    0 0.5 0.25 0 0 0.10 0.15\n",
    "\n",
    "This gives the modeler great flexibility with employers or any population\n",
    "\n",
    "The matrix of usage PU looks like p rows and l columns\n",
    "\n",
    "    Pop_to_usage_pl_arr.shape = [ p, l ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IJKgtT_mL9W",
    "colab_type": "text"
   },
   "source": [
    "# Model Class: Each population's required resources per unit: `Population.resource_pn`, $P_R(p, n)$\n",
    "\n",
    "This is the first derived figure taking in the above variables\n",
    "\n",
    "So with this, you can see that with a single operation you can get to the actual equipment levels require per person per day for a given population. Note that there is new Python 3.5 syntax for [matrix multiply](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465)\n",
    "\n",
    "    $PU x UR = PU[p, l] x UR[l, n] = P_R[p, n]$\n",
    "    # in the new Python 3.5 syntax using ampersand\n",
    "    # np.dot for matrices but not tensors\n",
    "    P^R = PU @ UC\n",
    "    # Which is the same as but in shorter notation\n",
    "    P^R = PU.dot(UC)\n",
    "\n",
    "In Python Numpy speak, we are doing a matrix [multiply](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)\n",
    "\n",
    "    # Now you can see the value of the suffix\n",
    "    # It is a quick check because for a matrix multiply to work\n",
    "    # you need the column of the first to match the row of the second\n",
    "    # Note how with a matrix, both the name `use` and the subscript\n",
    "    # `l` must match for this to work properly\n",
    "    Pop_to_res_pn_df = Pop_to_usage_pl_df @ Usage_to_res_ln_df\n",
    "\n",
    "Note that this is still in \"per capita terms\" that is we are getting usage per person and have not gotten to the total population yet.\n",
    "\n",
    "### v2.1 Extend to multiple burn rates $UR_{dln}$ or `Usage_to_res_dln[d, l, n]`\n",
    "In v2.1,  When this is extended with different burn rates this becomes a multi-dimensional array.\n",
    "\n",
    "The key concept is that PU x UR is really an assumption about the per capita rate is 1, so with d=1, it simplifies to just $UR(l, n)$, but in v2.1, you have for each detail as different run rate so the usage matrix has a new dimension UR[d, l, n] or for each detail, there is a different consumption level l for every resource item n.\n",
    "\n",
    "Assuming we have to do the PU[p, l] across then we need to broadcast this multiply across UR[1, p, n], UR[2, p, n],... And then sum it which is easy to do with an einsum:\n",
    "\n",
    "    # v2.1 with details\n",
    "    $PU x UR = PU[p, l] x UR[d, l, n] = P_R[d, p, n]$\n",
    "    # In python this is easiest to do as an einsum\n",
    "    Pop_resource_dpn = np.einsum(\"pl,dln=dln\", Pop_to_usage_pl, Usage_to_res_dln)\n",
    "\n",
    "With UR, you can create specific levels like EMT for instance with different runs and you can have some based on number of EMTs, another d could be number of runs per day handled or number of COVID-19 patients or number of suspected cases. This gives you great flexibility in assignment of run rates. You don't have to convert everything to per capita numbers.\n",
    "\n",
    "Most of these matrices will be very sparse, but it removes so much specific logiv and replaces it with very fast matrix multiplies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_So41jQQXVE",
    "colab_type": "text"
   },
   "source": [
    "# Moving Beyond Surge in v2: Social Mobility $SM_t$ and Economic Activity $EA_t$\n",
    "\n",
    "There are four major goals for V2.x:\n",
    "\n",
    "Time series forecast based of what's needed so that all matrices now have a time dimension so you can see things change with time.\n",
    "\n",
    "Explicit utilization models beyond per capita the two major ones are\n",
    "    - per \"run or 911 call\" for things that are incident based. This means that every population is going to have an activity matrix in addition to the population matrix\n",
    "    - per COVID PUD (Patient Under investigation) mainly for testing tempos so there will be a matrix of how many patients there are in a given population\n",
    "\n",
    "The time series particularly for activity is based on social mobility model that is handled off this model, but comes in as a matrix SM[p, t] which is how each social mobility population changes for time indexed on 100% at Feb 7.\n",
    "\n",
    "Time series that is the disease progression from SIR or other Epi model which modeled as DP[p, t] so you can look at various populations with differential infection rates.\n",
    "\n",
    "## The first step extending with an time with Social mobility and Economic Recovery\n",
    "\n",
    "This is first step, the basic strategy is to take the Population needs P[p, n] and add a third dimension PE[t, p, n] where t is the time (arbitrarily we are saying weeks for now, but could be days).\n",
    "\n",
    "So the basic input into this most simplistic model, we assume we get a vector that shows social mobility as a function of time for each population $SM[p, t]$ where each row is a population percentage of recovery, that is if say Feb 7 2020 was 100% activity in a given population, then we see if the first population doesn't start until week 3 and then ramps, mobility would look like if say non-healthcare was row 0 and healthcare workers are row 1\n",
    "\n",
    "    [ [ 0.0, 0.0, 0.3, 0.4 ],\n",
    "      [ 0.4, 0.5, 0.6, 0.7 ]]\n",
    "\n",
    "Generating this social mobility projection is easy backwards looking with things like the IHME social index, the trick is a simple projection forward. \n",
    "\n",
    "In this model, we will by default use a simple [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) to model this as a placeholder and start them at different points so the Social Mobility Model Input parameters (thank goodness for [Latex Markdown](https://en.wikibooks.org/wiki/LaTeX/Mathematics), [Medium](https://towardsdatascience.com/latex-for-data-scientists-in-under-6-minutes-3815d973c05c), [Latex4Technics](https://www.latex4technics.com/?note=gw021j)\n",
    "\n",
    "While there are many types of [Sigmoids](https://www.quora.com/Is-there-any-difference-between-sigmoid-logistic-and-tanh-function) like tanh, we will use the logistic since it is commonly used in machine learning. The logistic function originally came from population research and it is theoretically [optimal](https://www.quora.com/q/kvuotomuzenzevuw/Logistic-Softmax-Regression) if we ever apply optimization to the problem\n",
    "\n",
    "So for every population we end with a simple starter S[p, 2] which carries the three parameters a or the maximum height, and b the start offset. This is convenient encapsulated in [Scipy Logistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.logistic.html)\n",
    "\n",
    "$S(a, b, t) = a \\frac{1}{1 + e^{-t}} + b$\n",
    "\n",
    "From this its easy to generate the actual SM[p, t] = S(a, b, t)\n",
    "\n",
    "Or in real python where a is scale and bj is loc for location\n",
    "\n",
    "    from scipy.stats import logistic\n",
    "    logistic.cdf(t, loc=0, scale=2)\n",
    "\n",
    "With this we can then generate the Essentials Items as E[p, n, t] which is then just that for each slice E[p, n, 0] = E[p, n] * SM[p, 0].values with casting\n",
    "\n",
    "This can be vectorized as well E[p, n] * S[p, t] = ET [p, n, t]as an example with the simplest example of two populations where the non-healthworkers use 500 non-ASTM masks and the healthcare workers use 100 at full surge, \n",
    "\n",
    "    [[ 0, 500],\n",
    "     [ 100, 0 ]]\n",
    "\n",
    "Then if the start sequence looks like the non-healthworker are idle in the first week and then go to 50% in the second, while healthcare workers start at 30% and go to 70%\n",
    "\n",
    "    [ [ 0, 0.5 ],\n",
    "      [ 0.3, 0.7]]\n",
    "\n",
    "The math looks like doing this with a broadcast, so you extend E first with a stack ET[p, n, t ] = E[p, n].stack(t) so you get identical\n",
    "\n",
    "Then you can do element wise math\n",
    "\n",
    "### The Economic recovery\n",
    "\n",
    "Using Fed data, they do model today a U-shaped recession so we can model the amount of economic activity in addition to the amount of social mobility. The social mobility affects the customers coming into stores and demand for social services. The Recovery data gives a sense of the amount of business that a given store will have.\n",
    "\n",
    "## The next step carrying different run rates\n",
    "\n",
    "First we are explicitly going to change the modeling to not just carry Population[p], but actually a bunch of factors including that are for each population, Population[p, a] includes a bunch of attributes and then when you calculate burn rates, you sum across all of the\n",
    "\n",
    "$PR_pn = P_{pa} \\, PR^T_{apn}$\n",
    "\n",
    "    $Population_use_tpn = np.einsum(\"pa,tp\", Population_pa_df, Social_mobility_tp)$\n",
    "\n",
    "So what is in the characteristic C[p, m]\n",
    "  * Number of COVID-19 Patients in a give population for healthcare usage based on patients\n",
    "  * Number of Patients Under Investigation\n",
    "  * Number of Activities by the population which is a different tempo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-qdjB5KkmMV",
    "colab_type": "text"
   },
   "source": [
    "# Total (per period) required by a population equipment: `Population.total_unit_use_pn[p, n]`, $T^P_{pn}$\n",
    "\n",
    "Now that we have the per-capita requirements, we need to do a scalar multiply by row. As an asside, if you don't want to do the\n",
    "\n",
    "    $T_P(p, n) = P_R(p, n) x P(p, 1)$\n",
    "    # Or in python using broadcasting which extends P out n columns\n",
    "    $T_P = P_R * P$\n",
    "    # in Dataframe, you get scalar multiply by converting to np array values\n",
    "    # Note we are only plucking out the percapita number here.\n",
    "    Total_pop_unit_res_pn_df = Pop_res_pn_df * Population_p.values\n",
    "\n",
    "\n",
    "## v2.2 Adding details to $P^T_{dpn}$ or Population.total_use_dpn[d, p, n]\n",
    "In the case where we have details and run then across, we just extend the details, we now have we extend to details with a broadcast, note that using einsum makes this easy so we don't first have to transpose\n",
    "\n",
    "    $P_T(p, n) = P_R(d,p,n) x P(p,d).T$\n",
    "    Pop_total_res_pn_df = Pop_res_dpn_tf * Population_pd.values.T\n",
    "    # this is equivalent and probably easier to read\n",
    "    Pop_total_res_pn_df = np.einsum(\"dpn,pd=pn\", Pop_res_dpn_tf, Population_pd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B61cEeUjma-2",
    "colab_type": "text"
   },
   "source": [
    "# Essentiality Class Transform: `Essential.to_pop_ep[e, p]`, $EP^T(e, p)$\n",
    "\n",
    "Many times the subpopulations are going to be too large to understand. For instance when there are 800 job classfied by SOC or where there are 350 employer class by NAICS-6, so for convenience, we define essential levels. You can think of the of this as for each population, where do they fit in where they start. Essential (which has reversed so 0 is the highest since version 1.x) can be thought of as the time period of start. So Essential 0 (like Defcon 1), is the most important and so forth. \n",
    "\n",
    "There are two uses of essentiality. The first is to compress very large populations into something that is understandable. If you have say 800 different classes of workers such as the US Standard Occupational Codes (SOC), then this is too much to display so you might have essential levels like healthcare worker, or blue collar worker. In the v1.x models there were seven levels:\n",
    "\n",
    "- No Protection\n",
    "- Residential\n",
    "- White collar worker (in an office setting)\n",
    "- Customer facing (in contact with many others)\n",
    "- Blue collar worker (manufacturing, construction)\n",
    "- Potential contact healthcare worker\n",
    "- In contact with COVID healthcare worker\n",
    "\n",
    "## v2.2 The other use for stage restart `Essential.time_te` or $T^E_{te}$\n",
    "\n",
    "This extends easily to staged restart, so for the example essentiality levels of non-healthcare employed and heathcare employed, it might look like a simple matrix across 6 start periods as or more analytically E is e rows and t columns.\n",
    "\n",
    "So in the example, it says the first population starts at time 0 and then the second starts in week 6, so in a t, e, at time zero e0 starts and then six weeks late e1 starts\n",
    "\n",
    "    1 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 1\n",
    "\n",
    "But this system also allows a staged restart, so for example, if you want have the workers to come back in the next period for healthcare employees and this series could even be generated as a lambda with any arbitrary function, so in this example, population 1 starts 50% in week 0, then slows continues. \n",
    "\n",
    "While population 2 doesn't start until week 4 and tails up\n",
    "\n",
    "    0.5 0\n",
    "    0.4 0\n",
    "    0.1 0\n",
    "    0 0\n",
    "    0 0.2\n",
    "    0 0.4\n",
    "    0 0.6\n",
    "\n",
    "The module has to create this. In the test model, these are hard coded, but in reality, this is tied to a government's start plan and needs to pull from a database. Create a new object for each with parameters as needed to get the right.\n",
    "\n",
    "This will let us extend the requirements out in time easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCjHZ9bRmfhL",
    "colab_type": "text"
   },
   "source": [
    "# Population to essentiality `Ess_res_en` $E^R_{en}$\n",
    "\n",
    "In some sense we are doing compression by this, so we are looking at Essential index e is much less than the number of populations p. Or more succinctly e << p and we can get to E with a transpose\n",
    "\n",
    "$EP^T_{ep} * P^R{pn} = e x p * p x n = E^R{en}$\n",
    "\n",
    "    # In python this looks like\n",
    "    E_R = EP^T @ P_R\n",
    "    Ess_res_en_df = Essential_to_pop_ep_df @ Pop_res_pn_df\n",
    "\n",
    "## In v2.1 Population to essentialy over time `Ess_res_ten[t, e, n]` $E^R_{ten}$\n",
    "\n",
    "This is this we are extneding the essentiality over time by stacking\n",
    "\n",
    "    Ess_res_ten_df = Einsum(\"tn, en= ten\", Time_by_ess_tn_df, Ess_res_en_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqiejks7lR6g",
    "colab_type": "text"
   },
   "source": [
    "# Supply Class: Cost for essential levels for every item: `Supply.cost_essential_en`, $C^E_{en}$\n",
    "\n",
    "For simplicity we can assume that each essential level has different costs. In reality, the costs will actually be more complicate and C[p, n] which is much more complicated we leave out as p can be very large and e is usually a small number of levels usual less than 10.\n",
    "\n",
    "For each item, what is the cost for N95 is $3 and non-surgical is $0.50 for high volume say healthcare workers and then higher costs for the rest of the population so there might be way s=3 scenarios\n",
    "\n",
    "    Cost_essential_en[e, n] = \n",
    "              [[ $3, $0.50 ],\n",
    "               [ $4.5, $1.0]]\n",
    "\n",
    "\n",
    "### v2.1.2 Adding a r range of costs `Supply.cost_essential_ren[r, e, n]`, $C(r, e, n)$\n",
    "Without the time components, if you add a range, you get a range of costs\n",
    "\n",
    "Later on as an example, although this should really use iterators to generate it since this is just 50% more for each\n",
    "see https://djangostars.com/blog/list-comprehensions-and-generator-expressions\n",
    "\n",
    "    $C^E_{ren} = C^{low}_{en}, C^{mid}_{en}, C^{hi}_{en}$\n",
    "\n",
    "    Cost_essetial_ren[r, e, n] = [[[$2, $0.25],\n",
    "                   [$3, $0.50]],\n",
    "                  [[$3, $0.50],\n",
    "                   [$4.5, $1]],\n",
    "                  [[$5, $1]\n",
    "                   [$6, $2]]]\n",
    "\n",
    "### v2.2 Adding time and the supply model 'Supply.cost_essential_tren[t,r,e,n)\n",
    "For the surge model, we just look at the peak required, but in fact we are going to get orders and then a series of prices and quantities over time t so for every time period, we get different costs for each essential level and each resource tiem\n",
    "\n",
    "    Cost_ess_res_tren[r, t, e, n] = The cost at time t for every time\n",
    "\n",
    "## Daily cost calculations $DC^E_{en}$ `Supply.daily_cost_ess_en[e, n]`\n",
    "\n",
    "OK that was the hard stuff, with these matrices reduced to essential levels and the equipment needed for each. If you now have the units needed for each essential level per period (usually day), then there are some other things you want and this is a simple scaler multiply\n",
    "\n",
    "    $DC^E_{en} = C^E_{en} * E^R_{en}$\n",
    "    Daily_cost_ess_en = Cost_essential_en_df * Ess_res_en_df.values\n",
    "\n",
    "So we have $DC^E_{en}$ which is the total units needed, now we need to do the cost analysis assuming we have variable costs\n",
    "\n",
    "### v2.1 Get a range of costs\n",
    "This is basically using casting to get Ed_{en} broadcase across all the Cost range dimension. We need to make sure this broadcast works \n",
    "\n",
    "    # Note we reverse this, assuming that broadcast has to happen\n",
    "    Daily_cost_ess_ren = np.einsum(\"ren,en=ren\", Cost_essential_ren_df * Ess_res_en_df.values\n",
    "\n",
    "### v2.2 Extent this over time\n",
    "\n",
    "    Daily_cost_ess_tren = np.einsum(\"tren,en=tren\", Cost_essential_tren_df * Ess_res_en_df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcQFx6VAlSX5",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "## Model: Stockpile need by essential levels is `Resource.stockpile_en`, $S[e,n]$\n",
    "The second analysis has to do with warehouse needs and sensitivity\n",
    "\n",
    "- Stockpile estimates for say a 30-day stockpile\n",
    "- Volume estimates for daily use\n",
    "\n",
    "So for each essential you need a different stockpile. Usually more essential needs more levels. This is really another tensor.\n",
    "\n",
    "We are doing this to allow a simple Einsum to calculate all of these rather than one at a time. See the sections below for the non-tensor calculation, but the tensor one looks like\n",
    "\n",
    "    S[d, e, n ] = Einsum(\"ed, en = den\", R[e, n], Resources[n, d])\n",
    "    Stockpile needed for d Days = S[e, n] = R[e, n] * SE[e, 1].value\n",
    "\n",
    "This let's you calculate lots of parameters at the same time giving you a cost range, stockpile ranges and volumes for warehouse calculations\n",
    "\n",
    "Obviously you may not want to stock pile for all e Essential levels, so you just select what you want for instance S[0] will give you the stockpile needs for the most essential level 0.\n",
    "\n",
    "## Gross Cost for equipment by level Resource.total_cost_en\n",
    "\n",
    "So both the cross cost and the stockpile are done by level as a element-wide multiplication.\n",
    "\n",
    "    Gross cost for the equipment = RC[e, n] = R[e, n] * C[e, n].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o_lYYWBgOsH",
    "colab_type": "text"
   },
   "source": [
    "# Detail of Model\n",
    "\n",
    "These are the details of the model. It is a good example of the parameters that you will need to add. Make sure that you have good advice from medical authorities when looking over these parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TU4PflJ5b2zK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPZYjkTlg8CS",
    "colab_type": "code",
    "cellView": "form",
    "colab": {}
   },
   "source": [
    "# https://colab.research.google.com/notebooks/forms.ipynb#scrollTo=ZCEBZPwUDGOg\n",
    "#@title Basic Model Parameters\n",
    "#@markdown ####Enter Model Description here:\n",
    "\n",
    "model_name = 'NYC Surge Forecast'  #@param\n",
    "model_description = 'v1.4 WHO Surge'  #@param {type: \"string\"}\n",
    "recurrence_index = 45  #@param {type: \"slider\", min: 0, max: 100}\n",
    "recovery_index = 62  #@param {type: \"slider\", min: 0, max: 100}\n",
    "revision =   103#@param {type: \"number\"}\n",
    "date = '2020-05-20'  #@param {type: \"date\"}\n",
    "model_type = \"surge\"  #@param ['surge', '3-month', '6-month']\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ####Daily Usage of Equipment Per Person\n",
    "units = \"10,000\" #@param [\"1,000,000\", \"100,000\", \"10,000\", \"1,000\"] {allow-input: true}\n",
    "\n",
    "#@markdown ---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLFSWaOVi7tR",
    "colab_type": "text"
   },
   "source": [
    "## Daily Usage of Equipment __n__ by Protection levels __l__ is D[l, n]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1xwe8g08yRbG",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "544be766-28ad-4de0-93f0-c47d97063c70"
   },
   "source": [
    "# Eventually we will do this from a database import, but for now, let's use\n",
    "# the data that is normally in the Excel sheet and just recreate \n",
    "# https://colab.research.google.com/drive/1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW?authuser=5#scrollTo=1xwe8g08yRbG\n",
    "\n",
    "# Using PEP https://www.python.org/dev/peps/pep-0008/\n",
    "# For simplicity do as a dictionary\n",
    "Item_name = [\n",
    "              'N95 Surgical Respirator',\n",
    "              'N95 Mask',\n",
    "              'ASTM 3 Surgical Mask',\n",
    "              'ASTM 1-2 Surgical Mask',\n",
    "              'Non-ASTM Mask'\n",
    "              'Reusable Cotton Mask'\n",
    "              'Cotton Mask with Ear Loop',\n",
    "              'Face Shield',\n",
    "              'Goggles'\n",
    "              'Gown',\n",
    "              'Gloves',\n",
    "              'Shoe Covers',\n",
    "              'Test Kits',\n",
    "              'Disinfectant (30ml)',\n",
    "              'Disinfectant wipes'\n",
    "            ]\n",
    "\n",
    "# For this demo, we will just test with two\n",
    "Level_name = [ 'WA0', 'WA1', 'WA2', 'WA3', 'WA4', 'WA5', 'WA6']\n",
    "Item_name = [ 'N95 Surgical', 'non ASTM Mask']\n",
    "print('Item_names', Item_name)\n",
    "Daily_usage_matrix = [\n",
    "                [ 0, 0 ],\n",
    "                [ 0, 1 ],\n",
    "                [ 0, 2 ],\n",
    "                [ 0.1, 3],\n",
    "                [ 0.2, 4],\n",
    "                [ 0.3, 6],\n",
    "                [ 1.18, 0]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFHD-0I8JU8N",
    "colab_type": "text"
   },
   "source": [
    "### Daily Usage Matrix verification and conversion to Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2uM7R8IXJTQE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "outputId": "2bbb7a50-b242-4cb2-f593-6b45a2d3244c"
   },
   "source": [
    "print('Daily_usage_matrix', Daily_usage_matrix)\n",
    "\n",
    "Daily_usage_df = pd.DataFrame(Daily_usage_matrix,\n",
    "                              columns = Item_name,\n",
    "                              index = Level_name)\n",
    "\n",
    "# use these counts to check the matrix vector bugs\n",
    "level_count = Daily_usage_df.shape[0]\n",
    "item_count = Daily_usage_df.shape[1]\n",
    "print('usage_pd shape is ', Daily_usage_df.shape,\n",
    "      'protection level count is ', level_count,\n",
    "      'item count is ', item_count)\n",
    "\n",
    "print('Daily_usage_pd', Daily_usage_df)\n",
    "\n",
    "Daily_N95s_usage = Daily_usage_df['N95 Surgical']\n",
    "print('Daily N95 Surgical Usage', Daily_N95s_usage)\n",
    "\n",
    "# https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
    "print('Daily usage value in Dataframe', Daily_usage_df.values)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4uMRbROcqpf",
    "colab_type": "text"
   },
   "source": [
    "## Population Data by sub-populations p is P[p, 1]\n",
    "\n",
    "Start with the simplest assumption, two populations, one that is `WA6` and one that is `WA2` as an example. But we will insert more data later once we decide the data source."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odyDmPbkc3l2",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "23b47a1b-9c4d-409a-b9e2-a7fc7183f806"
   },
   "source": [
    "# This is a dummy test case, later we will use extraction first form a\n",
    "# spreadsheet and then eventually from a data store that is reliable\n",
    "# And which has revision control\n",
    "\n",
    "Population_name = ['Healthcare employees', 'Non employees of healthcare companies']\n",
    "Population_data = [735.2, 7179.6]\n",
    "\n",
    "print('Population Data', Population_data)\n",
    "\n",
    "Population_df = pd.DataFrame(Population_data, index = Population_name, columns = ['Population'])\n",
    "population_count = Population_df.shape[0]\n",
    "print('population count p', population_count)\n",
    "print(Population_df)\n",
    "\n",
    "# https://note.nkmk.me/en/python-type-isinstance/\n",
    "print('type of Population_name', type(Population_name))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hqG2MmDelJZs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxOgyfxTet7N",
    "colab_type": "text"
   },
   "source": [
    "# Usage of PPE by Sub-population p is U[p, l]\n",
    "\n",
    "Now we have a vector which are the population usages and we have a list of needs, so we need to do a matrix multiply of population by needs. Each entry is the percentage of a population at a given level.\n",
    "\n",
    "So in this example, 50% of healthcare workers are level 5 and 50% are at level 6"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NA65fr95e8w0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "outputId": "edd86d25-7d12-498f-b261-c7d39353d064"
   },
   "source": [
    "# Now we need a matrix which is the pop_type x usage_type and the coefficient is just how much is needed for each\n",
    "# Do this for simplicity start with a zero matrix, we will actually load the data\n",
    "\n",
    "Usage_by_population_matrix = np.zeros([population_count, level_count])\n",
    "\n",
    "Usage_by_population_matrix[1,1] = 1.0\n",
    "Usage_by_population_matrix[0,6] = Usage_by_population_matrix[0, 5] = 0.5\n",
    "print('Usage_by_population_matrix', Usage_by_population_matrix)\n",
    "\n",
    "# https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/\n",
    "Usage_by_population_df = pd.DataFrame(Usage_by_population_matrix,\n",
    "                                      index = Population_name,\n",
    "                                      columns = Level_name)\n",
    "print(Usage_by_population_df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGfH93RCkjXk",
    "colab_type": "text"
   },
   "source": [
    "# Required Equipment n per capita per sub-population p per capita is R[p, n]\n",
    "\n",
    "This is the first multiplication where we take the two matrices and multiply them together. So this will give us a matrix. Each row is for the populations and then each column shows the daily usage by population."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rUb-b6EPlUre",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "outputId": "62664228-007a-4816-c4a9-bc0cfc6b8021"
   },
   "source": [
    "print('Daily_usage_df', Daily_usage_df.shape)\n",
    "print('Usage_by_population_df', Usage_by_population_df.shape)\n",
    "\n",
    "# Note with Panda multiply the index of rows and the columns have to match\n",
    "Required_df = Usage_by_population_df @ Daily_usage_df\n",
    "\n",
    "print('Required_df', Required_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmHR1tXJMYW0",
    "colab_type": "text"
   },
   "source": [
    "# Total Required equipment for each Population T[p, n]\n",
    "\n",
    "We are now just going to case the Population count vector across the required per capita to get the total required across all populations. So we need [element-wise multiplication](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy) which is denoted and this works because of casting, so P is duplicated for each column. Th syntax is different in each variant, for [Dataframes](https:/stackoverflow.com/questions/21022865/pandas-elementwise-multiplication-of-two-dataframes_)\n",
    "\n",
    "    # In Matlab\n",
    "    R .* P \n",
    "    # In Numpy\n",
    "    R * P\n",
    "    # In \n",
    "    R * P.values\n",
    "\n",
    "This is a pretty easy calculation, you just need the element-wise multiplication of the actual population numbers against the per-capita needs. Because of t he way broadcasting works, the vector is spread properly\n",
    "\n",
    "$T[p, n] = R[p, n] * P[p, 1].values$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BZn5jauDNHgU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "outputId": "56da6b0f-906f-4d29-8ab4-7e322041605f"
   },
   "source": [
    "\n",
    "print('Required_df shape', Required_df.shape)\n",
    "print(Required_df)\n",
    "print('Population_df shape', Population_df.shape)\n",
    "print(Population_df)\n",
    "\n",
    "Total_required_df = Required_df * Population_df.values\n",
    "print(Total_required_df)\n",
    "\n",
    "# another formulation\n",
    "Total_items_per_population_df = Required_df * Population_df.values\n",
    "print('Total items for each subpopulation')\n",
    "print(Total_items_per_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjU4CuZGYi_w",
    "colab_type": "text"
   },
   "source": [
    "# Now convert Population rows into Essential rows with E[e, p]\n",
    "This changes the labels and let's you assign each Population with an Essentiality index. Eventually, the essentiality will represent a time series. So e=0 means start at week 0 (for healthcare) and then e=N means start at week N. \n",
    "\n",
    "We can even do a time series on that too, say have a sigmoid for the starting or some other sort of lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CvE_lclnbL4q",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "cdd6c4e6-cd89-4e11-e736-7891e775bd17"
   },
   "source": [
    "# this is a square inverse as we healthcare works are week 0 and non-healthcare is week 1\n",
    "\n",
    "Essential_name = [ \"Essential\",\n",
    "                   \"Non-essential\"]\n",
    "\n",
    "Essential_by_population_matrix = [\n",
    "                                    [1, 0],\n",
    "                                    [0, 1]                                 \n",
    "                                  ]\n",
    "\n",
    "Essential_by_population_df = pd.DataFrame(Essential_by_population_matrix,\n",
    "                                      index = Essential_name,\n",
    "                                      columns = Population_name)\n",
    "\n",
    "print('Essential by population', Essential_by_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HzfZeTtbRFY",
    "colab_type": "text"
   },
   "source": [
    "# Now use that matrix to convert Required equipment by population to Required by Essentiality\n",
    "\n",
    "So we that T[p, n] and we convert with another matrix multiple as\n",
    "\n",
    "    RE[e, n] = E[e, p] @ T[p, n] "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LX2LPIO2clDN",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "9b3a339a-7a6d-4f87-99e7-d3615756a979"
   },
   "source": [
    "Required_by_essential_df = Essential_by_population_df @ Total_required_df\n",
    "print('Require by Essential Index', Required_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hj_MLrCc7zZ",
    "colab_type": "text"
   },
   "source": [
    "# Cost matrix is the Cost per item for all items C[e, n]\n",
    "\n",
    "For each row, we get a cost for the item, so for intance, if N95 surgicals are $3 and non-ASTM disposables are $0., the the vector looks like, because of broadcasting, if you just define a single row, it will be copied against all esesential levels\n",
    "\n",
    "    CE[e, n] =[ $3.00, $0.50]\n",
    "\n",
    "This should be the same width as the Product slicing.\n",
    "\n",
    "In this simple case, all costs across all essentials are identical, but in the more sophisticated costs, costs will vary by volume, so cost becomes a different across different essential levels as volumes and purchasing power are different\n",
    "\n",
    "    CE[e, n] = [ [ $3, $0.20 ],\n",
    "                [ $5, $0.50 ]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_qpsY-HvdGDf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "52b58639-6a53-4d4e-f556-f791ce22cbf0"
   },
   "source": [
    "# dataframe is a matrix\n",
    "# Assumes the same price for all users\n",
    "# Note that to make the math work, it need to a Numpy Array\n",
    "Cost_per_item_by_essential_matrix = np.array([ 3, 0.5] )\n",
    "\n",
    "# Assumes a different price depending on the essnetial level is 50% more\n",
    "Cost_per_item_by_essential_matrix = [ Cost_per_item_by_essential_matrix, Cost_per_item_by_essential_matrix * 1.5 ]\n",
    "Cost_per_item_by_essential_df = pd.DataFrame(Cost_per_item_by_essential_matrix,\n",
    "                                index = Essential_name,\n",
    "                                columns = Item_name)\n",
    "print('Cost per item',Cost_per_item_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUF3Zt2imJiA",
    "colab_type": "text"
   },
   "source": [
    "# Now calculate the costs based on Requirements and Cost matrix\n",
    "\n",
    "this is just the element-wise multiplication\n",
    "\n",
    "TE[e, n] = RE[e, n] * CE[e, n].value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1A-BZeN-pJ-u",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "aac88fde-8409-40f0-a62b-80f64d56f362"
   },
   "source": [
    "Total_cost_by_essential_df = Required_by_essential_df * Cost_per_item_by_essential_df.values\n",
    "print('Total cost by essential_df', Total_cost_by_essential_df )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dNjaRk-p87e",
    "colab_type": "text"
   },
   "source": [
    "# Finally use the Stockpile in days per essential to get the Stockpile by essential population\n",
    "\n",
    "This is another element wise multiply\n",
    "\n",
    "S[e, n] = RE[e, n] * DE[1, n].values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0BcGolJqS83",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "65da56ce-a1ae-413d-cadc-8bb847c9f110"
   },
   "source": [
    "# how much stockpile per item is needed for each level\n",
    "Day_stockpile_by_essential_matrix = [ [30], \n",
    "                                     [0]]\n",
    "Day_stockpile_by_essential_df = pd.DataFrame(Day_stockpile_by_essential_matrix,\n",
    "                                             index= Essential_name)\n",
    "\n",
    "print('Day_stockpile_by essential', Day_stockpile_by_essential_df,)\n",
    "\n",
    "# use .to_numpy as clearer than .values but this does not work\n",
    "Stockpile_by_essential_df = Required_by_essential_df * Day_stockpile_by_essential_df.values\n",
    "\n",
    "print('Stockpile by essential', Stockpile_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6tdby_AHdre",
    "colab_type": "text"
   },
   "source": [
    "# Estimating the Behavior over time BM[p, t] by population\n",
    "So this is a flatten matrix for each population, what happens over time. So for instance 1.0 means the same activity level as pre-COVID-19. You can have more than 1.0 if there is a frenzy of activity as an example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1QuIZ5y6yesE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "baf460a8-aeef-438c-d466-d22cca760830"
   },
   "source": [
    "# An example of a hard coded matrix without parameters\n",
    "Behavioral_time_by_population_matrix = [ [ 0.5, 1, 1],\n",
    "                                         [ 0, 0, 1]]\n",
    "Period_name = [ 'Week 1', 'Week 2', 'Week 3']\n",
    "Behavioral_time_by_population_df = pd.DataFrame(Behavioral_time_by_population_matrix,\n",
    "                                                index=Population_name,\n",
    "                                                columns=Period_name)\n",
    "print('Behavioral Time Parms')\n",
    "print(Behavioral_time_by_population_df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQPcQVkTVGDd",
    "colab_type": "text"
   },
   "source": [
    "# A matrix approach to stockpile\n",
    "Instead of separate calculations for each we create a details for the Resource is a vector Rd[unit, stockpile-30, stockpile-60, stockpile-90] In this way with a single multiple of Rd[p, d]\n",
    "\n",
    "So to calculate the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhJXjmqHzBkk",
    "colab_type": "text"
   },
   "source": [
    "# Creating a time series for Resources needed R[p, n] from BM[p, t] to Rtime[p, n, t]\n",
    "\n",
    "You can take any matrix and make it time oriented. So let's take the essential Stockpile and vary it by time.\n",
    "\n",
    "What is easiest to do with the time series feature of is to flatten all the matrices. And then it seems you just need to add a timestamp.\n",
    "\n",
    "The other apporach is to use the so called multiindex which let's you do multi dimensionals in a 2-D way.\n",
    "\n",
    "The use of iterables makes it easy to mix the labels in [multiindexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) but this is really a display thing. You cannot easily take a multidimensional cube and do a multiply. so the plan is to convert the multi-index to a numpy tensor do the math and then put it back.\n",
    "\n",
    "But the easier way is to do this as a pure matrix with numpy thanks to [Stackoverflow](https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays) the key is something called einsum which does give you a simple way to transform\n",
    "\n",
    "    np.einsum('ij, jk -> ijk', A, B)\n",
    "    # this basically extends the A[i,j] into the k dimension which is what we want where i=p, j=n, k=t\n",
    "    # these need to be in alphabetical order \n",
    "    np.einsum('ij,ik -> ijk ', R, BM)\n",
    "\n",
    "The second problem is how to make a multi-dimensional matrix (a tensor) displayable with Pandas, the answer seems to be a [multiIndex](https://stackoverflow.com/questions/43921419/transforming-multiindex-to-row-wise-multi-dimensional-numpy-array) which you create by cleaver reshaping, that is a multi-dimensional is basically a tensor unwrapped to a single vector, so you need to reshape it for a 2-D, it looks like\n",
    "\n",
    "    m,n = len(df.index.levels[0]), len(df.index.levels[1])\n",
    "    arr = df.values.reshape(m,n,-1).swapaxes(1,2)\n",
    "\n",
    "You can see how this becomes completely general as pseudocode where you get the lenght of each index and then reshape it\n",
    "\n",
    "    dimensions = len(df.index.levels)\n",
    "    tensor = df.values.reshape(dimensions,-1).swapaxes(reverse(dimensions))\n",
    "\n",
    "Or even more elegant [stackoverflow](https://stackoverflow.com/questions/35047882/transform-pandas-dataframe-with-n-level-hierarchical-index-into-n-d-numpy-array) points out that if you just create the right shaped numpy array, you can pour it in with flatten and this is real code, note the cool use of the map function\n",
    "\n",
    "    # create an empty array of NaN of the right dimensions\n",
    "    shape = map(len, df.index.levels)\n",
    "    array = np.full(shape, np.nan)\n",
    "    # fill it using Numpy's advanced indexing\n",
    "    array[df.index.labels] = df.values.flat\n",
    "\n",
    "The conversion from numpy array (tensor) to Pandas MultiIndex [stackoverflow](https://stackoverflow.com/questions/43427189/3-dimensional-numpy-array-to-multiindex-pandas-dataframe/48755377) uses the Panel\n",
    "\n",
    "    df = pd.Panel(arr).to_frame()\n",
    "\n",
    "But this is [deprecated](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Panel.html) since a panel only works for 3 dimensionals, but we can use this just to do the conversion easily.\n",
    "\n",
    "The basic strategy seems to be to flatten the numpy array and then create the index as the unrolled names. \n",
    "\n",
    "    df = pd.Series(arr, index=unrolled names)\n",
    "\n",
    "[Stackoverflow](https://riptutorial.com/pandas/example/6439/create-a-sample-dataframe-with-multiindex) explains that multiindex is really just a different index and you just stick it onto a vectors, so the idea is to take the first N-1 dimensions and make them a multi-index and then the last index (the column) can still sit there.\n",
    "\n",
    "    # you can create the index just by giving the labels in two sets\n",
    "    idx = pd.MultiIndex.from_product([['bar', 'baz', 'foo'],['one','two']])\n",
    "    # Then you can have columns which are the data itself\n",
    "    df = pd.DataFrame(np.random.randn(8, 2), index=idx, columns=['A', 'B'])\n",
    "\n",
    "The final piece of the puzzle is using [Pandas Time Series](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html) so you can utter things like this to create an index on time\n",
    "\n",
    "    index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "    data = pd.Series([0, 1, 2, 3], index=index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DThfVZtG4Z7S",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "062ab217-90cb-4d56-8320-f68cc50e7fb0"
   },
   "source": [
    "# https://stackoverflow.com/questions/25440008/python-pandas-flatten-a-dataframe-to-a-list\n",
    "# Not the right approach what we want is a stack of R[p, n]\n",
    "# When R[p, n] * BM[p, 1] => Rt[p, n .* BM[p,1]]\n",
    "R = np.array([[ 0, 100],\n",
    "              [50, 0]])\n",
    "print('Test R', R)\n",
    "BM = np.array ([ [ 0, 1],\n",
    "                 [ 1, 1]])\n",
    "print('Test BM', BM)\n",
    "Rt=np.zeros((2,2,2))\n",
    "Rt1=np.zeros((2,2))\n",
    "print('Rt', Rt)\n",
    "# https://stackoverflow.com/questions/4455076/how-to-access-the-ith-column-of-a-numpy-multidimensional-array\n",
    "# get the column vector for time 0 note that [0] means return a column vector not a row vector\n",
    "BM0=BM[:,[0]]\n",
    "print('BM0', BM0)\n",
    "print('BM0.shape', BM0.shape)\n",
    "\n",
    "# This should broadcast so R is p xn and BM0 is p x 1\n",
    "Rt0 = R * BM0\n",
    "print('Rt[:,:,0]', Rt0)\n",
    "\n",
    "print('Total Resource by population rp x n')\n",
    "print(Total_required_df)\n",
    "print('shape', Total_required_df.shape)\n",
    "\n",
    "print('Behavorial model p x t')\n",
    "print(Behavioral_time_by_population_df)\n",
    "print('shape', Behavioral_time_by_population_df.shape)\n",
    "\n",
    "# Now the magic note the output is in alphabetical order\n",
    "# https://ajcr.net/Basic-guide-to-einsum/\n",
    "# Total_required_df i rows, j columns; Behavorial_time i row, k column\n",
    "ijk=np.einsum('ij, ik -> ijk', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('ijk', ijk.shape, ijk)\n",
    "\n",
    "kji=np.einsum('ij, ik -> kji', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('kji', kji.shape, kji)\n",
    "print('kji[0]', kji[0])\n",
    "\n",
    "kij=np.einsum('ij, ik -> kij', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('kij', kij.shape, kij)\n",
    "print('kij[0]', kij[0])\n",
    "\n",
    "# p = population, n = items, t = time\n",
    "Total_required_by_time_matrix = np.einsum('pn, pt -> tpn', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('Total required over time')\n",
    "print(Total_required_by_time_matrix)\n",
    "\n",
    "# Test conversion to a Pandas MultiIndex\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "# you can create the index just by giving the labels in two sets\n",
    "idx = pd.MultiIndex.from_product([['bar', 'baz', 'foo', 'fob'],['one','two']])\n",
    "print('idx', idx)\n",
    "# Then you can have columns which are the data itself\n",
    "df = pd.DataFrame(np.random.randn(8, 2), index=idx, columns=['A', 'B'])\n",
    "print('df', df)\n",
    "\n",
    "# first create dates\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\n",
    "Time_name = pd.date_range(start='6/10/2020', periods=3, freq='W')\n",
    "print('Time', Time_name)\n",
    "print('Population names', type(Population_name), Population_name)\n",
    "Total_required_by_time_index = pd.MultiIndex.from_product([Time_name, Population_name]) \n",
    "print('Indices', Total_required_by_time_index)\n",
    "\n",
    "# flatten out all but the last index\n",
    "last_dim = Total_required_by_time_matrix.shape[-1]\n",
    "print('size of the last dimension', last_dim)\n",
    "print('size of the time total', Total_required_by_time_matrix.size)\n",
    "# Reshape the matrix to be 2D, not you need to force the division to be an\n",
    "# integer\n",
    "Total_required_by_time_2d = Total_required_by_time_matrix.reshape(\n",
    "    (int(Total_required_by_time_matrix.size/last_dim),\n",
    "    last_dim))\n",
    "\n",
    "# Note you cannot pass 3D matrices, so flatten it into 2D\n",
    "Total_required_by_time_df = pd.DataFrame(Total_required_by_time_2d,\n",
    "                                         index=Total_required_by_time_index,\n",
    "                                         columns=Item_name)\n",
    "\n",
    "print('Totel', Total_required_by_time_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmzf8K60yA7a",
    "colab_type": "text"
   },
   "source": [
    "# Behavioral modeling BM[p, t] from parameters BMp[p, 2]\n",
    "This long term will be a Python function from a PIP library which will also get instantiated as a REST API so it can be called. (see the section on implementation\n",
    "\n",
    "But this implements the time series of restart across a set of populations. The interface is the Dataframe BM[p, t] which is a factor for how much activitiy there is the Population matrix. So this model says the first population, the non-healthcare workers start 40% in week 2 and then get to 60% by week 3.\n",
    "\n",
    "The healthcare workers start and this is manually encoded.\n",
    "\n",
    "For simplicity, you can use a generator function as well to create this so you don't have to create a gigantic table by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7NW8qt4SN6U",
    "colab_type": "text"
   },
   "source": [
    "## Using parameters to create the activity matrix\n",
    "This is harder than it looks, but the idea is to use some simple functions and generate a matrix. The main problem here is that you want to do it quickly, so ideally, you would use vector match to create all the matrices automatically as a template\n",
    "\n",
    "The first attempt with numpy.fromfunction sort of works, but you cannot parameterize them very well, so the solution is to use vectorize as a way to get it across an entrie array.\n",
    "\n",
    "But since this varries by row, instead, we use from function for each row and then can concatenate them together"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ye7uD8BvSWUE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "8b9c5747-05e3-4e72-e5bf-5b3d2c509e33"
   },
   "source": [
    "# How to create an model automagically with just paramaters\n",
    "# Remember Python doesn't have arrays, it uses lists for that\n",
    "# https://www.programiz.com/python-programming/matrix\n",
    "# And index syntax is completely different from numpy\n",
    "Behavioral_by_population_parameter_matrix = [ ['logistic', 1, 0.7],\n",
    "                                              ['logistic', 0, 1 ] ]\n",
    "Logistic_parameter_name = [ 'function', 'loc', 'scale']\n",
    "\n",
    "Behavioral_by_population_parameter_df = pd.DataFrame(Behavioral_by_population_parameter_matrix,\n",
    "                                                     index=Population_name,\n",
    "                                                     columns=Logistic_parameter_name)\n",
    "\n",
    "print(Behavioral_by_population_parameter_df)\n",
    "print('scipy stats')\n",
    "from scipy.stats import logistic \n",
    "for t in range(1,3):\n",
    "  print(t, logistic.cdf(t, loc=1.4, scale=2.0))\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
    "# Can use a function to create a long list\n",
    "print(type(Behavioral_by_population_parameter_matrix))\n",
    "print(Behavioral_by_population_parameter_matrix)\n",
    "\n",
    "# remember Python matrices index from 0\n",
    "print(Behavioral_by_population_parameter_matrix[0][1],\n",
    "      type(Behavioral_by_population_parameter_matrix[0][2]))\n",
    "\n",
    "# https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types\n",
    "# need to turn them into real numbers\n",
    "value=Behavioral_by_population_parameter_matrix\n",
    "\n",
    "# Note that From function is not called once per cell, it is called exactly once and not once per cell\n",
    "# You need to use np.vectorize to push the function across all cell elements\n",
    "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
    "\n",
    "# https://stackoverflow.com/questions/50997928/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-1d/50997969\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=1.2,\n",
    "                                           scale=2.3\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This works, so you can do a lookup from a global variable inside a lambda\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
    "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "\n",
    "# now try using p and this works fine\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=p,\n",
    "                                           scale=p+1\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This does not work, the error is only integer scalar arrays can be converted to a scalar index \n",
    "print('getting just p+t')\n",
    "# This works because you are actually doing array match, p is (2,3) and t is (2, 3)\n",
    "print(np.fromfunction(lambda p, t : p+t, (2, 3)))\n",
    "print('value of p and t')\n",
    "print(np.fromfunction(lambda p, t : p, (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : t, (2, 3)))\n",
    "\n",
    "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
    "# Explains what is going on \n",
    "\n",
    "print('type of p and t')\n",
    "# Unintuitive, what you get is not a scalar in p, but a numpy array when in the above you\n",
    "# just get a scalar, why is this?\n",
    "# The function actually gets an input that is the shape noted so it is (2,3)\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
    "print(np.fromfunction(lambda p, t : type(p), (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : type(t), (2, 3)))\n",
    "\n",
    "# This demonstrates that what you get is every possible point\n",
    "# so p will be all the row numbers which are the same across all the columns\n",
    "# and t will be the column indices which are the same across all columns\n",
    "# which is not what stackoverflow said\n",
    "# So to use this, you need a function which does array math properly\n",
    "def show(p, t):\n",
    "  print('show')\n",
    "  print('called value of p')\n",
    "  print(type(p), p.shape, p)\n",
    "\n",
    "  print('called value of t')\n",
    "  print(type(t), t.shape, t)\n",
    "\n",
    "print(np.fromfunction(show, (3, 3)))\n",
    "\n",
    "print('shapes of p and t')\n",
    "print(np.fromfunction(lambda p, t : p.shape, (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : t.shape, (2, 3)))\n",
    "\n",
    "# This works because logistics handles arrays, so takes all of t and applies it\n",
    "# The problem is that loc and scale need scalars, so you can index it with p, which is a matrix\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
    "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This will not work because you are getting p as an array and t as an array and it expects\n",
    "# a array function\n",
    "# print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "#                                           loc=Behavioral_by_population_parameter_matrix[p][1],\n",
    "#                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "#                                           ), (2, 3)))\n",
    "\n",
    "# The right approach is to just use the working code above but apply it to each row and then glom the whole thing together\n",
    "# https://www.pluralsight.com/guides/numpy-arrays-iterating\n",
    "# This works because numpy treats a matrix actually as a collection of lists so very elegant\n",
    "print(Behavioral_by_population_parameter_df)\n",
    "print(Behavioral_by_population_parameter_matrix)\n",
    "\n",
    "# Iterating over a 2-D dataframe which is much more complicated than a matrix\n",
    "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "# https://www.geeksforgeeks.org/different-ways-to-iterate-over-rows-in-pandas-dataframe/\n",
    "# note that in Dataframes, the rows are actually behind, so it is column major\n",
    "print('iterate over rows in dataframe')\n",
    "for population in Behavioral_by_population_df.index:\n",
    "  print('population', type(population), population)\n",
    "  print(Behavioral_by_population_df['Week 1'][population])\n",
    "\n",
    "print('iterate using iterrows')\n",
    "for index, population in Behavioral_by_population_df.iterrows():\n",
    "  print(index, type(population), population['Week 1'])\n",
    "\n",
    "print('iterate of rows in numpy matrix')\n",
    "for population in Behavioral_by_population_matrix:\n",
    "  # you will get each item overall\n",
    "  print(population)\n",
    "\n",
    "# Another approach is apply_along_axis which is really cool\n",
    "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
    "# \n",
    "\n",
    "# https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays\n",
    "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.outer.html\n",
    "# https://stackoverflow.com/questions/26089893/understanding-numpys-einsum\n",
    "# A completely different approach\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhXkKFfuW1Gy",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jggktC4lTNrs",
    "colab_type": "text"
   },
   "source": [
    "# Attachment: Test Code\n",
    "\n",
    "Used to test various features of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1uq0RMITXy3",
    "colab_type": "text"
   },
   "source": [
    "## Test of cloning an external Repo\n",
    "\n",
    "NOte that this does a complete clone in the virtual machine, make sure you have enough space. Also you need to reclone when you close a Notebook instance, so this can be slow with lots of data.\n",
    "\n",
    "However, it does allow you checkout particular branches and have a realiable dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kD9qp04tU9-L",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Clone the entire repo.\n",
    "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
    "%cd cloned-repo\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDnksm1MTmV7",
    "colab_type": "text"
   },
   "source": [
    "## Test of copying a single file from a repo\n",
    "\n",
    "This one way to get small datasets, you just point to the raw file and use `!curl` to bring it into the machine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q7eqOYtkU99S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Fetch a single <1MB file using the raw GitHub URL.\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38KeztwBajb8",
    "colab_type": "text"
   },
   "source": [
    "## Test of connecting to Google Drive\n",
    "\n",
    "This we can use if we don't need a repo, but are just loading a static file. We normally want everything from a repo or reliable storage, but this is good for quick analysis. In most cases, you should just check this into a repo and then use the github raw extract instead so you get version control.\n",
    "\n",
    "Note that this does require an authentication everytime you start the Notebook, so the raw extract works better particularly if there it is a public repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pKyKRJeoHtJ_",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "161cf9d1-0eda-4bc5-c4ee-9e60bf6a0a36"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2ldUhSATDRY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "636d4080-ca9f-4c69-e8bf-1da9c9b5baba"
   },
   "source": [
    "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive!')\n",
    "!cat '/gdrive/My Drive/foo.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXxIvzREazRq",
    "colab_type": "text"
   },
   "source": [
    "## Connecting two cells together for summaries with Cross-output Communications\n",
    "\n",
    "This is the best method for connecting the longer analysis to a cell that just has the executive summary data. _This does not appear to be working. Need to debug_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndtWkGTjlL70",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "97cb1602-94b0-4b95-eaa7-f1cb709686d2"
   },
   "source": [
    "%%javascript\n",
    "const listenerChannel = new BroadcastChannel('channel');\n",
    "listenerChannel.onmessage = (msg) => {\n",
    "  const div = document.createElement('div');\n",
    "  div.textContent = msg.data;\n",
    "  document.body.appendChild(div);\n",
    "};"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmwMtzEslL7J",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "a52b20de-e709-41d3-db8b-ef899208045e"
   },
   "source": [
    "%%javascript\n",
    "const senderChannel = new BroadcastChannel('channel');\n",
    "senderChannel.postMessage('Hello world!');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AulXwNtb5yd",
    "colab_type": "text"
   },
   "source": [
    "## Creating forms for entry\n",
    "\n",
    "This is going to be used to parameterize models. This sets global variables that can be used in cells farther down."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lXWX8aG7lWvD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Example form fields\n",
    "#@markdown Forms support many types of fields.\n",
    "\n",
    "no_type_checking = ''  #@param\n",
    "string_type = 'example'  #@param {type: \"string\"}\n",
    "slider_value = 142  #@param {type: \"slider\", min: 100, max: 200}\n",
    "number = 102  #@param {type: \"number\"}\n",
    "date = '2010-11-05'  #@param {type: \"date\"}\n",
    "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
    "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
    "#@markdown ---\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDR12BB4pPd1",
    "colab_type": "text"
   },
   "source": [
    "## Display Pandas data dataframes use Vega datasets as an example\n",
    "\n",
    "This uses the extension `google.colab.data_table` and there is a default data set called `vega_datasets` where you can extract data. It is not clear where the data is or how to figure out how ot use it. Google-fu does not help although the [source code](https://github.com/googlecolab/colabtools/blob/master/google/colab/data_table.py) tells us that `vega_dataset` has airport data in it.\n",
    "\n",
    "But the hing is in the name Vega which is a visualization package and [Vega Datasets access from Python](https://github.com/jakevdp/vega_datasets) are a standard set of data for visualization testing. The core datasets are kept in [github.io](https://vega.github.io/vega-datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wW43_ntJpdVh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%load_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knd4KPzcpdUf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "outputId": "51470f15-5595-4086-affc-1cbaa0e17de1"
   },
   "source": [
    "from vega_datasets import data\n",
    "data.cars()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwkagE3vpdTY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%unload_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKvLbYFyy8J-",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kgrbax2npdRf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "outputId": "dc2a8470-1228-417a-b29a-982a50d56c5f"
   },
   "source": [
    "data.stocks()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evpmsNgsOaF",
    "colab_type": "text"
   },
   "source": [
    "## Github Rendering of Jupyter\n",
    "\n",
    "This is pretty cool, but [Github](https://help.github.com/en/github/managing-files-in-a-repository/working-with-jupyter-notebook-files-on-github) actually renders the Jupyter notebooks as statis HTML when you browse it. That means just clicking on a `.ipynb` will give you something reasonable. It is not interactive nor is anything running behind it, but it does mean that documents produced by use are easily readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RV8Y3kpyklo",
    "colab_type": "text"
   },
   "source": [
    " "
   ]
  }
 ]
}
