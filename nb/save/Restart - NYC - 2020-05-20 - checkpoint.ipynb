{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copy of Restart - NYC - 2020-05-20.ipynb",
   "provenance": [
    {
     "file_id": "1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW",
     "timestamp": 1590965803841
    }
   ],
   "collapsed_sections": [
    "jggktC4lTNrs",
    "H1uq0RMITXy3",
    "MDnksm1MTmV7",
    "38KeztwBajb8",
    "gXxIvzREazRq",
    "7AulXwNtb5yd",
    "GDR12BB4pPd1"
   ],
   "toc_visible": true,
   "mount_file_id": "1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW",
   "authorship_tag": "ABX9TyNMJ6V4ogOgse9Jlr5QiBbj"
  },
  "kernelspec": {
   "name": "",
   "display_name": ""
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPP19bJbIKev",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "_Proprietary and Confidential. Do not distribute without permission._\n",
    "\n",
    "---\n",
    "\n",
    "# Restart Partners  \n",
    "\n",
    "---\n",
    "_To:_ Jun Amora (Mayors Office of the City of New York) \n",
    "_From:_ Bharat Shyam, Rich Tong  (Restart US)\n",
    "_Re:_ Analysis for NYC PPE needs  \n",
    "_Date:_ 20 May 2020  \n",
    "\n",
    "--- \n",
    "\n",
    "New York City needs a 90-day stockpile for the heathcare workers, first responders and congregate care facilities is really important, but coming up with an estimate for this is difficult given the variability of the infection and the uncertainty in the degree of economic recovery and social mobility. Therefore, we are providing another projection to augment yours that shows that our figures are within 30%-50% of your bottoms estimate. Given that we are happy to:\n",
    "\n",
    "- _Refine healthcare estimates_. All models are heavily dependent on estimates of population involved and usage data. \n",
    "- _Non-healthcare estimates_. For instance, this model does project needs outside of the healthcare area such as small business, vertical industries and vulnerable populations.\n",
    "- _Long-term modeling_. We are extending the model to include test equipment, disinfectant wipes and liquid disinfectants, so happy to add things that you need. Also we will be integrating epi and economic models too and would love to partner with you on that.\n",
    "\n",
    "Given the uncertainties involved, this might help you make the right estimates. What follows next are:\n",
    "\n",
    "1. Disclaimer. This is not a definitive estimate. You should use other sources and information to make your decisions.\n",
    "2. Data Sources. We have included the model source data, how the model is constructed and then results. Feel free to use this data and modify as appropriate, but it serves as documentation for all the assumptions made.\n",
    "3. Model. The way the calculation is done with assumptions and resulting projections\n",
    "4. Outputs. The conclusions we can draw from the projection.\n",
    "\n",
    "## Disclaimer\n",
    "It must be noted that the Restart Partners (\"Restart\") Equipment Model (the \"Model\") is made available for public use free of charge. Determining equipment needs for each jurisdiction, entity or other party (each a \"User\") is a complex and multifaceted decision process. Restart does not does not have the authority or ability to assign empirical risks levels nor make definitive use decisions for any User. Rather, the Model provides one approach to making recommendations that can help Users make decisions about their potential equipment uses by allowing them to calculate their potential requirements. Users are strongly encouraged to consider other sources of information and expressly disclaim any cause of action they may have against Restart arising from or relating to the Model or its analysis. Implementing the equipment levels projected by the Model will not eliminate the risk of COID-19 cases being linked to activites in an economy or workplace. In this context, it is important to note that this equipment alone will not eliminate the risk of infection. All Users should remain informed about and abide by any decisions made by local public health and government authorities regarding specific mitigation efforts, including equipment in the model, as the situation is dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xo1YI-GvU4o",
    "colab_type": "text"
   },
   "source": [
    "# Model Data\n",
    "\n",
    "Because we do not have New York City specific data, we used various open data sources to fill in the five major assumptions in the model:\n",
    "\n",
    "1. Usage by Population. This cuts the item usage per person per day. This right now is a series of levels. So we have four levels for civilians and then two levels for healthcare workers.\n",
    "2. Usage per Patient. This is the way Epidemiological models work. That is, given a number of patients, calculate how much they will need. The model currently uses the [WHO Surge Essential Supplies Forecasting Tool v1.2](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/covid-19-critical-items) and estimates the entire US population use with 1,000 cases and fast transmission and slow response. So this is a very pessimistic scenario. This makes sense when calculating the surge estimates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TU4PflJ5b2zK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOPqqurOE8-E",
    "colab_type": "text"
   },
   "source": [
    "# Strategy for the Model\n",
    "\n",
    "The next steps are a little complicated, but we are converting the entire model into a series of vectorized operations. \n",
    "\n",
    "## Daily Usage of Equipment by Protection levels D[l, n]\n",
    "\n",
    "So first we need the inputs which are the list of protection levels p x the number of items we are tracking n. So this is an p x n matrix where each entry says for protection level l, we have so many items per capita per day. So the first data list is Daily_usage D = l rows x n columns or D.shape = ( l, n )\n",
    "\n",
    "In Python speak this is Usage_pd since it is a _P_anda _D_ataframe. Right now this is a test matrix that is constructed in the Jupyter Notebook, but longer term, it should be pulled from an database with the suitable annotations.\n",
    "\n",
    "## Sub-Population Count vector P[p, 1]\n",
    "\n",
    "In the original model, we had p sub-populations. In the simplest model, it was just two populations: non-employees of healthcare companies and employees. With SOC and other codes, there are close to a thousands. So the populations are a vector that includes a description and a population count.\n",
    "\n",
    "So for example P = [ 7400, 435 ] which is just about the right numbers for Seattle and is a [ 1, p ] vector\n",
    "\n",
    "## Sub-Population Usage of Protection Levels U[p, l]\n",
    "\n",
    "You can think of this as a one-hot matrix in the most simple form. That is for each subpopulation, what level of protection do you need. \n",
    "\n",
    "For example, in the simplest case, if there are six protection levels, then if non-employees get level 1 and healthcare employees get level 6, then the matrix looks like:\n",
    "\n",
    "     0 1 0 0 0 0 0\n",
    "     0 0 0 0 0 0 1\n",
    "\n",
    "While date entry is complicated, this let's you take any given population and give it fractions of protection. For instance, if a healthcare employer typically had 50% of it's workers as office workers at level 2, 25% as customer facing and 10% taking care of non COVID and 15% in direct contact, that vector would look like:\n",
    "\n",
    "    0 0.5 0.25 0 0 0.10 0.15\n",
    "\n",
    "This gives the modeler great flexibility with employers or any population\n",
    "\n",
    "The matrix of usage U looks like p rows and l columns\n",
    "\n",
    "    Usage_pd.shape = [ p, l ]\n",
    "\n",
    "## Required Equipment per capita per sub-population R[p, n]\n",
    "\n",
    "So with this, you can see that with a single operation you can get to the actual equipment levels require per person per day for a given population. Note that there is new Python 3.5 syntax for [matrix multiply](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465)\n",
    "\n",
    "    U x D = [ p, l ] x [l, n] = R[p, n]\n",
    "    # in the new Python 3.5 syntax using ampersand\n",
    "    # np.dot for matrices but not tensors\n",
    "    R = U @ D\n",
    "    R = U.dot(D)\n",
    "\n",
    "In Python Numpy speak, we are doing a matrix [multiply](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)\n",
    "\n",
    "## Total required equipment for a sub-population T[p, n]\n",
    "\n",
    "Now that we have the per-capita requirements, we need to do a scalar multiply by row\n",
    "\n",
    "    R[p, n] x P[p, 1] = T[p, n]\n",
    "    # Or in python using broadcasting which extends P out n columns\n",
    "    T = R * P\n",
    "\n",
    "## Merging populations to with Essential index by population E[p, e]\n",
    "\n",
    "Many times the subpopulations are going to be too large to understand. For instance when there are 800 job classfied by SOC or where there are 350 employer class by NAICS-6, so for convenience, we define essential levels. You can think of the of this as for each population, where do they fit in where they start. Essential (which has changed since version 1.x) can be thought of as the time period of start. So Essential 0 (like Defcon 1), is the most important and so forth. \n",
    "\n",
    "This let's you stage start up, so for the example subpopulations of non-healthcare employed and heathcare employed, it might look like a simple matrix across 6 start periods as or more analytically E is p rows and e columns\n",
    "\n",
    "    0 0 0 0 0 0 1\n",
    "    1 0 0 0 0 0 0\n",
    "\n",
    "But this system also allows a stageed restart, so for example, if you want have the workers to come back in the next period for healthcare employees\n",
    "\n",
    "    0 0 0 0 0 0.5 0.5\n",
    "    0.5 0.5 0 0 0 0\n",
    "\n",
    "## Requirements by essential index R[e, n]\n",
    "\n",
    "In some sense we are doing compression by this, so we are looking at Essential index e is much less than the number of populations p. Or more succinctly e << p and we can get to E with a transpose\n",
    "\n",
    "    E[p, e].T x T[p, n] = e x p * p x n = R[e, n]\n",
    "    # In python this looks like\n",
    "    R = E.T @ T\n",
    "\n",
    "## The easy parts Required Cost per day RC[e, n] and Stockpile S[e, n]\n",
    "\n",
    "OK that was the hard stuff, with these matrices reduced to essential levels and the equipment needed for each, there are just some simple scalar multiplies to get where Cost is a row vector that is all the costs\n",
    "\n",
    "    Gross cost for the equipment = RC[e, n] = R[e, n] * C[1, n]\n",
    "    Stockpile needed for d Days = S[e, n] = R[e, n] * d\n",
    "\n",
    "Obviously you may not want to stock pile for all e Essential levels, so you just select what you want for instance S[0] will give you the stockpile needs for the most essential level 0.\n",
    "\n",
    "## Handling reuse and disinfection and reuse by essential level RE[e, n]\n",
    "\n",
    "This is a little tricky, but basically there is a vector that describes which items can be disinfected by what class. That is because some populations are easier to disinfect than others so this is a matrix of the major essential levels.\n",
    "\n",
    "For simplicity, we don't model it against every sub-population, but we could. So the Reuse by essential looks like\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1xwe8g08yRbG",
    "colab_type": "code",
    "outputId": "62693a71-e855-42f5-c648-adc28dcde386",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1590465051907,
     "user_tz": 420,
     "elapsed": 1008,
     "user": {
      "displayName": "Rich Tong",
      "photoUrl": "",
      "userId": "09447980366608024943"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    }
   },
   "source": [
    "\n",
    "# Eventually we will do this from a database import, but for now, let's use\n",
    "# the data that is normally in the Excel sheet and just recreate \n",
    "# https://colab.research.google.com/drive/1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW?authuser=5#scrollTo=1xwe8g08yRbG\n",
    "\n",
    "# Using PEP https://www.python.org/dev/peps/pep-0008/\n",
    "# For simplicity do as a dictionary\n",
    "usage_names = ['Protection Name',\n",
    "               'Source of Data',\n",
    "               'N95 Surgical Respirator',\n",
    "               'N95 Mask',\n",
    "               'ASTM 3 Surgical Mask',\n",
    "               'ASTM 1-2 Surgical Mask',\n",
    "               'Face Shield',\n",
    "               'Gown',\n",
    "               'Gloves',\n",
    "               'Shoe Covers',\n",
    "               'Test Kits',\n",
    "               ''\n",
    "               ]\n",
    "print(usage_names)\n",
    "usage_data = {'Source': [ 'WA0', 'WA1', 'WA2', 'WA3', 'WA4', 'WA5', 'WA6'],\n",
    "              'N95Surgical' : [ 0, 0, 0, 0, 0, 0, 1.18],\n",
    "              'N95' :[ 0, 0, 0, 0, 0.05, 0.10, 0],\n",
    "              }\n",
    "\n",
    "print(usage_data)\n",
    "\n",
    "usage_pd = pd.DataFrame(usage_data)\n",
    "# use these counts to check the matrix vector bugs\n",
    "pop_count = usage_pd.shape[0]\n",
    "item_count = usage_pd.shape[1]\n",
    "print('usage_pd shape is ', usage_pd.shape,\n",
    "      'population count is ', pop_count,\n",
    "      'item count is ', item_count)\n",
    "\n",
    "print(usage_pd)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4uMRbROcqpf",
    "colab_type": "text"
   },
   "source": [
    "## Get the population data\n",
    "\n",
    "Start with the simplest assumption, two populations, one that is `WA6` and one that is `WA2`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odyDmPbkc3l2",
    "colab_type": "code",
    "outputId": "4c796245-a272-4836-f3d2-0dc4e196c478",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1590464579318,
     "user_tz": 420,
     "elapsed": 3525,
     "user": {
      "displayName": "Rich Tong",
      "photoUrl": "",
      "userId": "09447980366608024943"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    }
   },
   "source": [
    "# This is a dummy test case, later we will use extraction first form a\n",
    "# spreadsheet and then eventually from a data store that is reliable\n",
    "# And which has revision control\n",
    "population_data = { 'pop_name' : ['Total less healthcare employees', 'Employees of healthcare companies'],\n",
    "                    'population' : [7179.6, 735.2]}\n",
    "\n",
    "print(population_data)\n",
    "\n",
    "pop_pd = pd.DataFrame(population_data)\n",
    "print(pop_pd)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxOgyfxTet7N",
    "colab_type": "text"
   },
   "source": [
    "# Estimate usage by population with a matrix multiply\n",
    "\n",
    "Now we have a vector which are the population usages and we have a list of needs, so we need to do a matrix multiply of population by needs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NA65fr95e8w0",
    "colab_type": "code",
    "outputId": "ef4f3a3e-4841-48b0-e204-1418475b9963",
    "executionInfo": {
     "status": "error",
     "timestamp": 1590464579328,
     "user_tz": 420,
     "elapsed": 3517,
     "user": {
      "displayName": "Rich Tong",
      "photoUrl": "",
      "userId": "09447980366608024943"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Now we need a matrix which is the pop_type x usage_type and the coefficient is just how much is needed for each\n",
    "pop_usage_matrix = np.zeros()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jggktC4lTNrs",
    "colab_type": "text"
   },
   "source": [
    "# Attachment: Test Code\n",
    "\n",
    "Used to test various features of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1uq0RMITXy3",
    "colab_type": "text"
   },
   "source": [
    "## Test of cloning an external Repo\n",
    "\n",
    "NOte that this does a complete clone in the virtual machine, make sure you have enough space. Also you need to reclone when you close a Notebook instance, so this can be slow with lots of data.\n",
    "\n",
    "However, it does allow you checkout particular branches and have a realiable dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kD9qp04tU9-L",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Clone the entire repo.\n",
    "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
    "%cd cloned-repo\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDnksm1MTmV7",
    "colab_type": "text"
   },
   "source": [
    "## Test of copying a single file from a repo\n",
    "\n",
    "This one way to get small datasets, you just point to the raw file and use `!curl` to bring it into the machine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q7eqOYtkU99S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Fetch a single <1MB file using the raw GitHub URL.\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38KeztwBajb8",
    "colab_type": "text"
   },
   "source": [
    "## Test of connecting to Google Drive\n",
    "\n",
    "This we can use if we don't need a repo, but are just loading a static file. We normally want everything from a repo or reliable storage, but this is good for quick analysis. In most cases, you should just check this into a repo and then use the github raw extract instead so you get version control.\n",
    "\n",
    "Note that this does require an authentication everytime you start the Notebook, so the raw extract works better particularly if there it is a public repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pKyKRJeoHtJ_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2ldUhSATDRY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive!')\n",
    "!cat '/gdrive/My Drive/foo.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXxIvzREazRq",
    "colab_type": "text"
   },
   "source": [
    "## Connecting two cells together for summaries with Cross-output Communications\n",
    "\n",
    "This is the best method for connecting the longer analysis to a cell that just has the executive summary data. _This does not appear to be working. Need to debug_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndtWkGTjlL70",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const listenerChannel = new BroadcastChannel('channel');\n",
    "listenerChannel.onmessage = (msg) => {\n",
    "  const div = document.createElement('div');\n",
    "  div.textContent = msg.data;\n",
    "  document.body.appendChild(div);\n",
    "};"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmwMtzEslL7J",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const senderChannel = new BroadcastChannel('channel');\n",
    "senderChannel.postMessage('Hello world!');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AulXwNtb5yd",
    "colab_type": "text"
   },
   "source": [
    "## Creating forms for entry\n",
    "\n",
    "This is going to be used to parameterize models. This sets global variables that can be used in cells farther down."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lXWX8aG7lWvD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Example form fields\n",
    "#@markdown Forms support many types of fields.\n",
    "\n",
    "no_type_checking = ''  #@param\n",
    "string_type = 'example'  #@param {type: \"string\"}\n",
    "slider_value = 142  #@param {type: \"slider\", min: 100, max: 200}\n",
    "number = 102  #@param {type: \"number\"}\n",
    "date = '2010-11-05'  #@param {type: \"date\"}\n",
    "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
    "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
    "#@markdown ---\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDR12BB4pPd1",
    "colab_type": "text"
   },
   "source": [
    "## Display Pandas data dataframes use Vega datasets as an example\n",
    "\n",
    "This uses the extension `google.colab.data_table` and there is a default data set called `vega_datasets` where you can extract data. It is not clear where the data is or how to figure out how ot use it. Google-fu does not help although the [source code](https://github.com/googlecolab/colabtools/blob/master/google/colab/data_table.py) tells us that `vega_dataset` has airport data in it.\n",
    "\n",
    "But the hing is in the name Vega which is a visualization package and [Vega Datasets access from Python](https://github.com/jakevdp/vega_datasets) are a standard set of data for visualization testing. The core datasets are kept in [github.io](https://vega.github.io/vega-datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wW43_ntJpdVh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%load_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knd4KPzcpdUf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from vega_datasets import data\n",
    "data.cars()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwkagE3vpdTY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%unload_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kgrbax2npdRf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data.stocks()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evpmsNgsOaF",
    "colab_type": "text"
   },
   "source": [
    "## Github Rendering of Jupyter\n",
    "\n",
    "This is pretty cool, but [Github](https://help.github.com/en/github/managing-files-in-a-repository/working-with-jupyter-notebook-files-on-github) actually renders the Jupyter notebooks as statis HTML when you browse it. That means just clicking on a `.ipynb` will give you something reasonable. It is not interactive nor is anything running behind it, but it does mean that documents produced by use are easily readable."
   ]
  }
 ]
}
