{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restart-model-readme-2020-07-19.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H1uq0RMITXy3",
        "MDnksm1MTmV7",
        "38KeztwBajb8",
        "gXxIvzREazRq",
        "7AulXwNtb5yd",
        "GDR12BB4pPd1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/restartus/covid-projection/blob/rich-demo/Restart_model_readme_2020_07_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPP19bJbIKev",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "_Proprietary and Confidential. Do not distribute without permission._\n",
        "\n",
        "---\n",
        "\n",
        "# Restart Partners   - Sample Memo\n",
        "\n",
        "---\n",
        "_To:_ Jun Amora (Mayors Office, City of New York)  \n",
        "_From:_ Bharat Shyam, Rich Tong (Restart)  \n",
        "_Re:_ Analysis for NYC PPE needs  \n",
        "_Date:_ 20 May 2020  \n",
        "\n",
        "--- \n",
        "\n",
        "New York City needs a 90-day stockpile for the heathcare workers, first responders and congregate care facilities is really important, but coming up with an estimate for this is difficult given the variability of the infection and the uncertainty in the degree of economic recovery and social mobility. Therefore, we are providing another resource model to augment yours that shows that our figures are within 30%-50% of your bottoms estimate. Given that we are happy to:\n",
        "\n",
        "- _Refine healthcare estimates_. All models are heavily dependent on estimates of population involved and usage data. \n",
        "- _Non-healthcare estimates_. For instance, this model does project needs outside of the healthcare area such as small business, vertical industries and vulnerable populations.\n",
        "- _Long-term modeling_. We are extending the model to include test equipment, disinfectant wipes and liquid disinfectants, so happy to add things that you need. Also we will be integrating epi and economic models too and would love to partner with you on that.\n",
        "\n",
        "Given the uncertainties involved, this might help you make the right estimates. What follows next are:\n",
        "\n",
        "1. Disclaimer. This is not a definitive estimate. You should use other sources and information to make your decisions.\n",
        "2. Data Sources. We have included the model source data, how the model is constructed and then results. Feel free to use this data and modify as appropriate, but it serves as documentation for all the assumptions made.\n",
        "3. Model. The way the calculation is done with assumptions and resulting projections\n",
        "4. Outputs. The conclusions we can draw from the projection.\n",
        "\n",
        "## Disclaimer\n",
        "It must be noted that the Restart Partners (\"Restart\") Equipment Model (the \"Model\") is made available for public use free of charge. Determining equipment needs for each jurisdiction, entity or other party (each a \"User\") is a complex and multifaceted decision process. Restart does not does not have the authority or ability to assign empirical risks levels nor make definitive use decisions for any User. Rather, the Model provides one approach to making recommendations that can help Users make decisions about their potential equipment uses by allowing them to calculate their potential requirements. Users are strongly encouraged to consider other sources of information and expressly disclaim any cause of action they may have against Restart arising from or relating to the Model or its analysis. Implementing the equipment levels projected by the Model will not eliminate the risk of COID-19 cases being linked to activites in an economy or workplace. In this context, it is important to note that this equipment alone will not eliminate the risk of infection. All Users should remain informed about and abide by any decisions made by local public health and government authorities regarding specific mitigation efforts, including equipment in the model, as the situation is dynamic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xo1YI-GvU4o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Model Data for New York use Washington Consumption\n",
        "\n",
        "Because we do not have New York City specific data, we used various open data sources to fill in the five major assumptions in the model:\n",
        "\n",
        "1. Usage by Population. This cuts the item usage per person per day. This right now is a series of levels. So we have four levels for civilians and then two levels for healthcare workers.\n",
        "2. Usage per Patient. This is the way Epidemiological models work. That is, given a number of patients, calculate how much they will need. The model currently uses the [WHO Surge Essential Supplies Forecasting Tool v1.2](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/covid-19-critical-items) and estimates the entire US population use with 1,000 cases and fast transmission and slow response. So this is a very pessimistic scenario. This makes sense when calculating the surge estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsdJwfq15v33",
        "colab_type": "text"
      },
      "source": [
        "# The Component architecture of the code classes\n",
        "\n",
        "The architecture of system models the three things happening in the real world which are the relationships between a population at the center. Here are the basic components of the v2 model and the graphical representation is below:\n",
        "\n",
        "1. Population. This contains all the data on the people under study. This includes the mapping of large population classes into a smaller set of essentiality levels for display \n",
        "2. Resource. Contains all the equipment needed to fight the pandemic and models how they are supplied including the inventory and stockpile levels.\n",
        "3. Disease. Contains all the disease modeling\n",
        "4. Economy. Contains the econometric and economic activity modeling\n",
        "5. Behavioral. How will people behave drives disease and recovery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK0tkgHU5Oaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "outputId": "e0e052ec-0a20-4fd7-e0f2-c5edc307129d"
      },
      "source": [
        "# https://h1ros.github.io/posts/introduction-to-graphviz-in-jupyter-notebook/\n",
        "# https://www.graphviz.org/pdf/dotguide.pdf for directed graphs manual\n",
        "# https://www.graphviz.org/pdf/neatoguide.pdf for undirected manual\n",
        "# https://graphviz.readthedocs.io/en/stable/manual.html\n",
        "from graphviz import Digraph, Graph\n",
        "dot = Digraph('Class Model', node_attr={'shape': 'box'})\n",
        "neato = Graph(engine='neato')\n",
        "\n",
        "# Population superclass using cluster as a special name\n",
        "# Note that name must be a named parameter\n",
        "# https://graphviz.readthedocs.io/en/stable/examples.html#cluster-py\n",
        "with dot.subgraph(name='Pop_class') as c:\n",
        "  c.attr(style='filled')\n",
        "  c.node('P', 'Population')\n",
        "\n",
        "dot.node('C', 'Consumption')\n",
        "dot.edge('P', 'C', 'Counts')\n",
        "\n",
        "dot.node('A', 'Activity')\n",
        "dot.edge('P', 'A', 'Counts')\n",
        "dot.edge('A', 'P', '%Normal')\n",
        "\n",
        "dot.node('D', 'Disease (Epidemiological)')\n",
        "dot.edge('D', 'P', 'Recovered')\n",
        "dot.edge('P', 'D', 'Cases')\n",
        "\n",
        "dot.node('E', 'Economics')\n",
        "dot.edge('E', 'P', 'GDP, Employment')\n",
        "dot.edge('P', 'E', 'Cost')\n",
        "\n",
        "dot.node('B', 'Behavior')\n",
        "dot.edge('P', 'B', 'Motivation, Ability, Prompt')\n",
        "dot.edge('B', 'P', 'Activity')\n",
        "\n",
        "with dot.subgraph(name=\"Res\") as c:\n",
        "  c.node('R', 'Resource')\n",
        "  c.edge('R', 'P', 'Supply')\n",
        "  c.edge('C', 'R', 'Burn Rates' )\n",
        "  c.node('I', 'Inventory')\n",
        "  c.edge('R', 'I', 'Fill')\n",
        "  c.edge('I', 'R', 'Use')\n",
        "  c.node('S', 'Supply')\n",
        "  c.edge('R', 'S', 'Sales Order')\n",
        "  c.edge('S', 'R', 'Fulfillment')\n",
        "\n",
        "# https://pypi.org/project/graphviz/\n",
        "# print the actual dot file\n",
        "print(dot.source)\n",
        "\n",
        "# now render in the notebook\n",
        "display(dot)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "digraph \"Class Model\" {\n",
            "\tnode [shape=box]\n",
            "\tsubgraph Pop_class {\n",
            "\t\tstyle=filled\n",
            "\t\tP [label=Population]\n",
            "\t}\n",
            "\tC [label=Consumption]\n",
            "\tP -> C [label=Counts]\n",
            "\tA [label=Activity]\n",
            "\tP -> A [label=Counts]\n",
            "\tA -> P [label=\"%Normal\"]\n",
            "\tD [label=\"Disease (Epidemiological)\"]\n",
            "\tD -> P [label=Recovered]\n",
            "\tP -> D [label=Cases]\n",
            "\tE [label=Economics]\n",
            "\tE -> P [label=\"GDP, Employment\"]\n",
            "\tP -> E [label=Cost]\n",
            "\tB [label=Behavior]\n",
            "\tP -> B [label=\"Motivation, Ability, Prompt\"]\n",
            "\tB -> P [label=Activity]\n",
            "\tsubgraph Res {\n",
            "\t\tR [label=Resource]\n",
            "\t\tR -> P [label=Supply]\n",
            "\t\tC -> R [label=\"Burn Rates\"]\n",
            "\t\tI [label=Inventory]\n",
            "\t\tR -> I [label=Fill]\n",
            "\t\tI -> R [label=Use]\n",
            "\t\tS [label=Supply]\n",
            "\t\tR -> S [label=\"Sales Order\"]\n",
            "\t\tS -> R [label=Fulfillment]\n",
            "\t}\n",
            "}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fd24dd199b0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Class Model Pages: 1 -->\n<svg width=\"865pt\" height=\"305pt\"\n viewBox=\"0.00 0.00 864.50 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n<title>Class Model</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-301 860.5,-301 860.5,4 -4,4\"/>\n<!-- P -->\n<g id=\"node1\" class=\"node\">\n<title>P</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"392.5,-297 312.5,-297 312.5,-261 392.5,-261 392.5,-297\"/>\n<text text-anchor=\"middle\" x=\"352.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Population</text>\n</g>\n<!-- C -->\n<g id=\"node2\" class=\"node\">\n<title>C</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"95,-210 0,-210 0,-174 95,-174 95,-210\"/>\n<text text-anchor=\"middle\" x=\"47.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Consumption</text>\n</g>\n<!-- P&#45;&gt;C -->\n<g id=\"edge1\" class=\"edge\">\n<title>P&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M312.4289,-276.4514C237.3174,-271.3254 81.2057,-258.888 62.5,-243 55.6745,-237.2027 51.8682,-228.4733 49.7729,-219.8673\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"53.2161,-219.2381 48.0209,-210.0045 46.324,-220.4625 53.2161,-219.2381\"/>\n<text text-anchor=\"middle\" x=\"83.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Counts</text>\n</g>\n<!-- A -->\n<g id=\"node3\" class=\"node\">\n<title>A</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"183,-210 120,-210 120,-174 183,-174 183,-210\"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activity</text>\n</g>\n<!-- P&#45;&gt;A -->\n<g id=\"edge2\" class=\"edge\">\n<title>P&#45;&gt;A</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M312.4888,-275.8514C247.3815,-270.3183 124.4277,-258.0022 111.5,-243 104.3983,-234.7587 108.3656,-225.4684 116.0541,-217.0967\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.6901,-219.4226 123.5839,-210.0258 113.8983,-214.3198 118.6901,-219.4226\"/>\n<text text-anchor=\"middle\" x=\"132.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Counts</text>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"371,-210 204,-210 204,-174 371,-174 371,-210\"/>\n<text text-anchor=\"middle\" x=\"287.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Disease (Epidemiological)</text>\n</g>\n<!-- P&#45;&gt;D -->\n<g id=\"edge5\" class=\"edge\">\n<title>P&#45;&gt;D</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M312.355,-272.62C294.8882,-267.6336 276.0373,-258.7641 265.5,-243 260.5943,-235.6609 262.0105,-227.0488 265.8546,-219.0128\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"268.9396,-220.672 271.0187,-210.2832 262.9149,-217.108 268.9396,-220.672\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cases</text>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"471.5,-210 391.5,-210 391.5,-174 471.5,-174 471.5,-210\"/>\n<text text-anchor=\"middle\" x=\"431.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Economics</text>\n</g>\n<!-- P&#45;&gt;E -->\n<g id=\"edge7\" class=\"edge\">\n<title>P&#45;&gt;E</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M365.0875,-260.8861C372.3775,-250.8358 381.974,-238.3199 391.5,-228 394.8111,-224.413 398.4369,-220.7905 402.111,-217.2965\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.6567,-219.7099 409.635,-210.3575 399.9111,-214.5641 404.6567,-219.7099\"/>\n<text text-anchor=\"middle\" x=\"405\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cost</text>\n</g>\n<!-- B -->\n<g id=\"node6\" class=\"node\">\n<title>B</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"700,-210 631,-210 631,-174 700,-174 700,-210\"/>\n<text text-anchor=\"middle\" x=\"665.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Behavior</text>\n</g>\n<!-- P&#45;&gt;B -->\n<g id=\"edge8\" class=\"edge\">\n<title>P&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M392.8288,-274.0641C432.1113,-268.6456 493.2322,-258.5853 544.5,-243 560.6923,-238.0776 563.9164,-234.6009 579.5,-228 593.0522,-222.2596 607.827,-216.0552 621.2462,-210.4407\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"622.8645,-213.5578 630.741,-206.4718 620.1648,-207.0994 622.8645,-213.5578\"/>\n<text text-anchor=\"middle\" x=\"660\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Motivation, Ability, Prompt</text>\n</g>\n<!-- R -->\n<g id=\"node7\" class=\"node\">\n<title>R</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"470,-123 401,-123 401,-87 470,-87 470,-123\"/>\n<text text-anchor=\"middle\" x=\"435.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Resource</text>\n</g>\n<!-- C&#45;&gt;R -->\n<g id=\"edge11\" class=\"edge\">\n<title>C&#45;&gt;R</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M95.2206,-177.9908C100.3647,-176.5927 105.5325,-175.2354 110.5,-174 209.9205,-149.2746 327.8261,-125.6445 390.7104,-113.4916\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.5305,-116.8981 400.6884,-111.5704 390.207,-110.0243 391.5305,-116.8981\"/>\n<text text-anchor=\"middle\" x=\"285.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Burn Rates</text>\n</g>\n<!-- A&#45;&gt;P -->\n<g id=\"edge3\" class=\"edge\">\n<title>A&#45;&gt;P</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M165.2336,-210.1227C174.573,-221.2129 187.86,-234.8019 202.5,-243 233.2558,-260.2226 272.0375,-269.2518 302.462,-273.9601\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"302.0389,-277.4352 312.4358,-275.3967 303.037,-270.5067 302.0389,-277.4352\"/>\n<text text-anchor=\"middle\" x=\"230.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">%Normal</text>\n</g>\n<!-- D&#45;&gt;P -->\n<g id=\"edge4\" class=\"edge\">\n<title>D&#45;&gt;P</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M294.0463,-210.4756C298.1571,-220.6389 304.0869,-233.1476 311.5,-243 314.2657,-246.6758 317.4575,-250.2525 320.8143,-253.6354\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"318.6411,-256.3962 328.3247,-260.6949 323.4354,-251.2957 318.6411,-256.3962\"/>\n<text text-anchor=\"middle\" x=\"342\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Recovered</text>\n</g>\n<!-- E&#45;&gt;P -->\n<g id=\"edge6\" class=\"edge\">\n<title>E&#45;&gt;P</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M432.1457,-210.0649C431.6479,-220.5871 429.4354,-233.5917 422.5,-243 417.0215,-250.4319 409.5592,-256.4485 401.5416,-261.2791\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"399.822,-258.23 392.6915,-266.0663 403.1524,-264.387 399.822,-258.23\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">GDP, Employment</text>\n</g>\n<!-- B&#45;&gt;P -->\n<g id=\"edge9\" class=\"edge\">\n<title>B&#45;&gt;P</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M700.035,-201.4776C726.77,-210.4559 756.7293,-225.1952 740.5,-243 718.1623,-267.5062 502.4587,-275.5913 402.6758,-278.0339\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.4462,-274.5382 392.5313,-278.2725 402.6109,-281.5363 402.4462,-274.5382\"/>\n<text text-anchor=\"middle\" x=\"768\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activity</text>\n</g>\n<!-- R&#45;&gt;P -->\n<g id=\"edge10\" class=\"edge\">\n<title>R&#45;&gt;P</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M470.359,-106.4806C578.0107,-112.1935 892.2573,-138.0609 795.5,-243 782.3438,-257.2687 516.1796,-271.3479 402.8685,-276.7202\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.54,-273.2316 392.7155,-277.1976 402.8689,-280.2239 402.54,-273.2316\"/>\n<text text-anchor=\"middle\" x=\"835\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Supply</text>\n</g>\n<!-- I -->\n<g id=\"node8\" class=\"node\">\n<title>I</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"429,-36 356,-36 356,0 429,0 429,-36\"/>\n<text text-anchor=\"middle\" x=\"392.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Inventory</text>\n</g>\n<!-- R&#45;&gt;I -->\n<g id=\"edge12\" class=\"edge\">\n<title>R&#45;&gt;I</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M400.674,-93.6126C389.3334,-88.0625 378.0003,-80.1145 371.5,-69 367.1221,-61.5145 368.4697,-52.9427 372.0398,-44.9899\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"375.218,-46.4773 377.0418,-36.0406 369.1077,-43.0621 375.218,-46.4773\"/>\n<text text-anchor=\"middle\" x=\"381.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fill</text>\n</g>\n<!-- S -->\n<g id=\"node9\" class=\"node\">\n<title>S</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"508,-36 449,-36 449,0 508,0 508,-36\"/>\n<text text-anchor=\"middle\" x=\"478.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Supply</text>\n</g>\n<!-- R&#45;&gt;S -->\n<g id=\"edge14\" class=\"edge\">\n<title>R&#45;&gt;S</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M443.742,-86.9655C448.2833,-77.1629 454.0834,-64.858 459.5,-54 460.9058,-51.1819 462.3979,-48.2541 463.9037,-45.3401\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"467.1353,-46.7134 468.6712,-36.2305 460.9333,-43.4676 467.1353,-46.7134\"/>\n<text text-anchor=\"middle\" x=\"493\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sales Order</text>\n</g>\n<!-- I&#45;&gt;R -->\n<g id=\"edge13\" class=\"edge\">\n<title>I&#45;&gt;R</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399.7161,-36.1819C403.7796,-46.0277 409.1097,-58.3268 414.5,-69 416.0005,-71.971 417.6397,-75.0367 419.3195,-78.067\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"416.3802,-79.9779 424.3818,-86.9223 422.4573,-76.5037 416.3802,-79.9779\"/>\n<text text-anchor=\"middle\" x=\"425.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Use</text>\n</g>\n<!-- S&#45;&gt;R -->\n<g id=\"edge15\" class=\"edge\">\n<title>S&#45;&gt;R</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M508.0614,-33.1818C522.6759,-42.7814 535.4549,-55.8273 526.5,-69 516.0401,-84.3866 497.8775,-93.1969 480.5161,-98.2417\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.2815,-94.944 470.4508,-100.798 481.0047,-101.7286 479.2815,-94.944\"/>\n<text text-anchor=\"middle\" x=\"561.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fulfillment</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ3osyCnkLWQ",
        "colab_type": "text"
      },
      "source": [
        "# A note on notation\n",
        "We have two sets of notation:\n",
        "\n",
        "- Formulaic notation using Latex that is suitable for math and proofs. It more concise, but derivable from and convertable to the long code names.\n",
        "- Notation for coding in Python. In this we are using some coding conventions as the terms can be confusing. We use snake_case for local variable and suffix each matrix or array with their dimensions. We use unique letters for various dimensions. For instance, we have p populations and n resources, so a matrix which is p rows and n columns would look like `data_pn_df` and then the last suffix is the data type\n",
        "\n",
        "## Einstein Summation\n",
        "Besides matrix multiplication and element-wise multiplication, we also make sure of [Einstein Summation](https://mathworld.wolfram.com/EinsteinSummation.html) popularized by Einstein's use in proving his theories, the notation allows axis summation, so that each index indicates what is iterated over and the left hand indicates what is summed or squashed\n",
        "\n",
        "$$\n",
        "P^C_{pn} = P_{pn}\\,PE^T_{tnp}\n",
        "$$\n",
        "\n",
        "This is exactly equivalent to \n",
        "\n",
        "    PT_tpn = np.einsum(\"pn,tnp=pn\",P_pn_df, T_tnp_df)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcywDAqH3z4h",
        "colab_type": "text"
      },
      "source": [
        "# Classes of the Model v2.x\n",
        "\n",
        "The general scheme is that there is a class Model which gets instantiated with all the appropriate labels. This provides the implicit dimensions of the model.\n",
        "\n",
        "## Model $M$ or `model`\n",
        "This is the core framework. It holds the dimensions and pointers to all the module component instances. It's main use is to \"dimension\" the problem so that the other subclasses know the size of the problem. It also holds points to the entire \"graph\" of objects (similar to a Keras object in machine learning. So the process is to add \"layers\" or model elements, then run the model as needed with a \"solver\" that is used for specific purposes and later for optimization of an objective function.\n",
        "\n",
        "Each major module can be subclassed from this and your can replace it. The current list of modules are and in the notation, the class name is the major part of the variable\n",
        "\n",
        "## Population as $P$ or `pop` with index `p`\n",
        "\n",
        "This holds the population and details about it. It's main output are two fold with a set of variables including:\n",
        "\n",
        "- Attributes $P^A_{pd}$ or `attr_pd_df` indexed by `d`. This includes in the first column for the number of people. \n",
        "- Protection Mappings $P^M_{pm}$ or `protection_pm_df` indexed by `m`. These are the levels for various works\n",
        "- Protection Demand $P^D_{mn}$ or `prot_demand_mn_df` which maps the `m` protection map to the actual consumption for each `n` resource\n",
        "- Population Level $P^L_{pl}$ or `level_pl_df` which maps a very large set of p population to a displayable set of `l` levels like essential and non-essential workers.\n",
        "\n",
        "## Resource $R$ or `Resource` with index `n`\n",
        "The resources needed, it takes demand from Population and returns to the population what can actualy be supplied.\n",
        "  - Attribute $R^A_{na}$ `attr_na_df` index with `a`\n",
        "  - Cost $R^C_{ln}$ `cost_ln_df` index with `ln` matrix which allows different \n",
        "  - Initial inventory $R^I_{ln}$ `initial_inventory_ln_df` for initial store\n",
        "  - Economic order quantity $R^E_{ln}$ `eoc_ln_df` for economic order quantity\n",
        "  - Safety stock $R^S_{ln}$ or `safety_stock_ln_df` for minimum inventory\n",
        "\n",
        "##Economy $E$ or `econ` for v3\n",
        "This is a model of the economy that takes in the Population and the degree of economic activity and returns the GDP and employment and other measures of work\n",
        "\n",
        "##Disease $D$ or `disease` for v3\n",
        "This models the progression of the disease and takes in the population and social mobility and it returns the number of patients, deaths and recovered patients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOPqqurOE8-E",
        "colab_type": "text"
      },
      "source": [
        "# Follow-on versions from v2 to v2.x\n",
        "\n",
        "The next steps are a little complicated, but we are converting the entire model into a series of vectorized operations. This documents:\n",
        "\n",
        "## v2. Duplicate the Excel Surge\n",
        "\n",
        "This duplicates the current surge model that is run by the Excel sheets as of v1.4.7. This is meant as a check of formulas and to ensure have duplicated the model.\n",
        "\n",
        "\n",
        "## v2.1 Add patients, more tests and tempos\n",
        "\n",
        "This includes calculations for adding patients counts and test tempos. This assumes that testing is either brought in as a separate module and the patients as well as a function of either SIR data or with some test method such as used by BMGF spreadsheets or WHO. It also adds burn rates by more than just daily per capita.\n",
        "\n",
        "## v2.2. Add ranges of costs\n",
        "\n",
        "In parenthesis are the notes for integrating the new features of v2.x. This includes a time series as the main features\n",
        "\n",
        "## v2.3 Add Time series\n",
        "\n",
        "So we do more than the surge model\n",
        "\n",
        "## v2.4 Think aboiut doing distributions rather than just ranges\n",
        "\n",
        "Would have a range of units increasing the dimensionality again. At this point, we might move to a mean with a sigma instead of many different parameters. This would mean we would have to do distribution math which asusming we have normal distributions is possible. For instance [normal distribution math](https://math.stackexchange.com/questions/2314763/variance-operations-on-the-normal-distribution) shows in [Latex](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)\n",
        "\n",
        "$$\n",
        "X = N(\\mu, \\sigma^2)\\\\\n",
        "Y = N(\\upsilon, \\tau^2)\\\\\n",
        "aX + bY = N(a\\mu + b\\upsilon, a^2\\sigma^2 +b^2\\tau^2)\n",
        "$$\n",
        "\n",
        "And I'm pretty sure this distribution math is a numpy function somewhere :-). However, the [product of two normal distributions](http://pldml.icm.edu.pl/pldml/element/bwmeta1.element.bwnjournal-article-doi-10_7151_dmps_1146/c/dmps.1146.pdf) is not normal, it is a Bessel function so it might be better just to use ranges which are a little easier to manage. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HROEvhCdl7m4",
        "colab_type": "text"
      },
      "source": [
        "# Additional extensions to move to time\n",
        "\n",
        "## Model Class\n",
        "\n",
        "v2.3. An idea is that when you change this, how does it dynamically recalculate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRPqcRMfklXE",
        "colab_type": "text"
      },
      "source": [
        "## Resource $R^A$ `attributes Notes\n",
        "\n",
        "This in the simplest form is just a vector of 1s with the labels being the materials.\n",
        "\n",
        "   Resource[n, 1] = np.ones((n, 1))\n",
        "\n",
        "We can handle any arbitrary vector of items, just change the list. Longer term, we will have our own GUID system, but right no rely on unique text strings\n",
        "\n",
        "### A note on handling conservation by amortizing specific items.\n",
        "\n",
        "In the v1.x models, we had a series of calculations indicating which items were reusable and which weren't. With an unlimited number of columns, it is much eaiser to just have dedicated items for:\n",
        "\n",
        "- Cloth masks\n",
        "- Disinfectable N95 respirators\n",
        "\n",
        "Then if an article lasts n days, set their tempo of use to 1/n, so a cloth mask that can be used for 10 days for disposal would be 0.1.\n",
        "\n",
        "This does not quite model the spike and flow, but is a decent approximation for now over many days.\n",
        "\n",
        "## Extending Physical facts about resources, `Resource.attr[d, n]`, $R_{dn}$ for v2.2\n",
        "\n",
        "\n",
        "We need to know some physical characteristics about resources. This could also come from Supply but will likely be more static. Some of the key ones are the volume and count\n",
        "\n",
        "    Resource_dn = [ units, square footage, weight, volume \n",
        "\n",
        "The other variables for the resource are physical characteristics like the volume of the package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohnMmgU5mANo",
        "colab_type": "text"
      },
      "source": [
        "## `Population.attr_pa` or $P_{pd}$ in v2.1 and higher\n",
        "v2.1 adds more population details d which can be for instance run rates for EMTs, number of potential cases in a class (althought this might move to the disease model) so this become `population_pd` $P(p,d)$)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUrTlN99mCpL",
        "colab_type": "text"
      },
      "source": [
        "# Population Demand $P^D_{dln}$ v2.\n",
        "Demand adds a detail level for multiple tempos `demand_res_dln[d, l, n]`, $P^OR_{dln}$\n",
        "v2.1 will add a population detail level so you can have tempos of things other than per capita, for instance, it could be per run or per COVID patient in the population and then it becomes `Usage_to_res_dln $C[d, n, l]` where d are the population Details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IJKgtT_mL9W",
        "colab_type": "text"
      },
      "source": [
        "# Model Class: Each population's demand for resources per capita: `Population.resource_pn`, $P^D_{pn}$\n",
        "This is the first derived figure taking in the above variables\n",
        "\n",
        "So with this, you can see that with a single operation you can get to the actual equipment levels require per person per day for a given population. Note that there is new Python 3.5 syntax for [matrix multiply](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465)\n",
        "\n",
        "$P^D_{pn} = P^M_{pm} x $P^{PD}_{mn}$\n",
        "\n",
        "In Python Numpy speak, we are doing a matrix [multiply](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)\n",
        "\n",
        "    # Now you can see the value of the suffix\n",
        "    # It is a quick check because for a matrix multiply to work\n",
        "    # you need the column of the first to match the row of the second\n",
        "    # Note how with a matrix, both the name `use` and the subscript\n",
        "    # `l` must match for this to work properly\n",
        "    population.demand_pn_df = protection_pm_df @ prot_demand_mn_df\n",
        "\n",
        "Note that this is still in \"per capita terms\" that is we are getting usage per person and have not gotten to the total population yet.\n",
        "\n",
        "## v2.1 Extend to multiple burn rates $P^D_{dln}$ or `demand_dpn`\n",
        "In v2.1,  When this is extended with different burn rates this becomes a multi-dimensional array.\n",
        "\n",
        "The key concept is that P^M @ P^PD is really an assumption about the per capita rate is 1, so with d=1, it simplifies to just $P^L_{mn}$, but in v2.1, you have for each detail as different run rate so the usage matrix has a new dimension UR[d, l, n] or for each detail, there is a different consumption level l for every resource item n.\n",
        "\n",
        "Assuming we have to do the PU[p, l] across then we need to broadcast this multiply across UR[1, p, n], UR[2, p, n],... And then sum it which is easy to do with an einsum:\n",
        "\n",
        "    # v2.1 with details\n",
        "    $PU x UR = PU[p, l] x UR[d, l, n] = P_R[d, p, n]$\n",
        "    # In python this is easiest to do as an einsum\n",
        "    population.demand_dpn = np.einsum(\"pl,dln=dln\", protection_pl_df, demand_dln)\n",
        "\n",
        "With UR, you can create specific levels like EMT for instance with different runs and you can have some based on number of EMTs, another d could be number of runs per day handled or number of COVID-19 patients or number of suspected cases. This gives you great flexibility in assignment of run rates. You don't have to convert everything to per capita numbers.\n",
        "\n",
        "Most of these matrices will be very sparse, but it removes so much specific logiv and replaces it with very fast matrix multiplies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_So41jQQXVE",
        "colab_type": "text"
      },
      "source": [
        "# Moving Beyond Surge in v2: Adding Disease $D$, Behavioral $B$ and Economics $E#\n",
        "\n",
        "There are four major goals for V2.x:\n",
        "\n",
        "Time series forecast based of what's needed so that all matrices now have a time dimension so you can see things change with time.\n",
        "\n",
        "Explicit utilization models beyond per capita the two major ones are\n",
        "    - per \"run or 911 call\" for things that are incident based. This means that every population is going to have an activity matrix in addition to the population matrix\n",
        "    - per COVID PUD (Patient Under investigation) mainly for testing tempos so there will be a matrix of how many patients there are in a given population\n",
        "\n",
        "The time series particularly for activity is based on social mobility model that is handled off this model, but comes in as a matrix $B_{tp}$ which is how each social mobility population changes for time indexed on 100% at Feb 7.\n",
        "\n",
        "Time series that is the disease progression from SIR or other Epi model which modeled as $D_{tp}$ so you can look at various populations with differential infection rates.\n",
        "\n",
        "## The first step extending with an time with Social mobility and Economic Recovery\n",
        "\n",
        "This is first step, the basic strategy is to take the Population demands for resources $P^D_{pn} and add a third dimension $P^D_{tpn} where t is the time (arbitrarily we are saying weeks for now, but could be days).\n",
        "\n",
        "So the basic input into this most simplistic model, we assume we get a vector that shows social mobility as a function of time for each population $B^P_{pt}$ where each row is a population percentage of recovery, that is if say Feb 7 2020 was 100% activity in a given population, then we see if the first population doesn't start until week 3 and then ramps, mobility would look like if say non-healthcare was row 0 and healthcare workers are row 1\n",
        "\n",
        "    [ [ 0.0, 0.0, 0.3, 0.4 ],\n",
        "      [ 0.4, 0.5, 0.6, 0.7 ]]\n",
        "\n",
        "Generating this social mobility projection is easy backwards looking with things like the IHME social index, the trick is a simple projection forward. \n",
        "\n",
        "In this model, we will by default use a simple [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) to model this as a placeholder and start them at different points so the Social Mobility Model Input parameters (thank goodness for [Latex Markdown](https://en.wikibooks.org/wiki/LaTeX/Mathematics), [Medium](https://towardsdatascience.com/latex-for-data-scientists-in-under-6-minutes-3815d973c05c), [Latex4Technics](https://www.latex4technics.com/?note=gw021j)\n",
        "\n",
        "While there are many types of [Sigmoids](https://www.quora.com/Is-there-any-difference-between-sigmoid-logistic-and-tanh-function) like tanh, we will use the logistic since it is commonly used in machine learning. The logistic function originally came from population research and it is theoretically [optimal](https://www.quora.com/q/kvuotomuzenzevuw/Logistic-Softmax-Regression) if we ever apply optimization to the problem\n",
        "\n",
        "So for every population we end with a simple starter S[p, 2] which carries the three parameters a or the maximum height, and b the start offset. This is convenient encapsulated in [Scipy Logistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.logistic.html)\n",
        "\n",
        "$B(t, a, b) = a \\frac{1}{1 + e^{-t}} + b$\n",
        "\n",
        "From this its easy to generate the actual $B_{tp} = B(a, b, t)$\n",
        "\n",
        "Or in real python where a is scale and bj is loc for location\n",
        "\n",
        "    from scipy.stats import logistic\n",
        "    logistic.cdf(t, loc=0, scale=2)\n",
        "\n",
        "With this we can then generate the Population demand for resources a $P^D_{tpn}$ which is tuples just that for each slice is just a casting multiply of $P^D_{0pn} = P^D_{pn} * B^P_{0pn} with casting\n",
        "\n",
        "This can be vectorized as well as an example with the simplest example of two populations where the non-healthworkers use 500 non-ASTM masks and the healthcare workers use 100 at full surge, \n",
        "\n",
        "    [[ 0, 500],\n",
        "     [ 100, 0 ]]\n",
        "\n",
        "Then if the start sequence looks like the non-healthworker are idle in the first week and then go to 50% in the second, while healthcare workers start at 30% and go to 70%\n",
        "\n",
        "    [ [ 0, 0.5 ],\n",
        "      [ 0.3, 0.7]]\n",
        "\n",
        "This is easy to do with an Einsum\n",
        "\n",
        "    population.level_demand_tln_df = einsum(\"pl,tpn=tln\",\n",
        "                                     population.level_pl_df\n",
        "                                     population.demand_tpn_df)\n",
        "\n",
        "Then you can do element wise math\n",
        "\n",
        "### The Economic recovery $E^P_{tpn}\n",
        "\n",
        "Using Fed data, they do model today a U-shaped recession so we can model the amount of economic activity in addition to the amount of social mobility. This is used to formulate the numerator, that is how much more GDP is there given the changes. The main inputs are the amount of economic activity being generated from $B$\n",
        "\n",
        "## The next step carrying different run rates\n",
        "\n",
        "First we are explicitly going to change the modeling to not just carry Population[p], but actually a bunch of factors including that are for each population, Population[p, a] includes a bunch of attributes and then when you calculate burn rates, you sum across all of the\n",
        "\n",
        "$PR_pn = P_{pa} \\, PR^T_{apn}$\n",
        "\n",
        "    $Population_use_tpn = np.einsum(\"pa,tp\", Population_pa_df, Social_mobility_tp)$\n",
        "\n",
        "So what is in the characteristic C[p, m]\n",
        "  * Number of COVID-19 Patients in a give population for healthcare usage based on patients\n",
        "  * Number of Patients Under Investigation\n",
        "  * Number of Activities by the population which is a different tempo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-qdjB5KkmMV",
        "colab_type": "text"
      },
      "source": [
        "# Total (per period) required by a population equipment: `Population.total_unit_use_pn[p, n]`, $T^P_{pn}$\n",
        "\n",
        "Now that we have the per-capita requirements, we need to do a scalar multiply by row. As an asside, if you don't want to do the\n",
        "\n",
        "    $T_P(p, n) = P_R(p, n) x P(p, 1)$\n",
        "    # Or in python using broadcasting which extends P out n columns\n",
        "    $T_P = P_R * P$\n",
        "    # in Dataframe, you get scalar multiply by converting to np array values\n",
        "    # Note we are only plucking out the percapita number here.\n",
        "    Total_pop_unit_res_pn_df = Pop_res_pn_df * Population_p.values\n",
        "\n",
        "\n",
        "## v2.2 Adding details to $P^T_{dpn}$ or Population.total_use_dpn[d, p, n]\n",
        "In the case where we have details and run then across, we just extend the details, we now have we extend to details with a broadcast, note that using einsum makes this easy so we don't first have to transpose\n",
        "\n",
        "    $P_T(p, n) = P_R(d,p,n) x P(p,d).T$\n",
        "    Pop_total_res_pn_df = Pop_res_dpn_tf * Population_pd.values.T\n",
        "    # this is equivalent and probably easier to read\n",
        "    Pop_total_res_pn_df = np.einsum(\"dpn,pd=pn\", Pop_res_dpn_tf, Population_pd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B61cEeUjma-2",
        "colab_type": "text"
      },
      "source": [
        "# Population to Poulation levels Class Transform: `Population.level_pl[p, l]`, $P^L(p, ;)$\n",
        "\n",
        "Many times the subpopulations are going to be too large to understand. For instance when there are 800 job classfied by SOC or where there are 350 employer class by NAICS-6, so for convenience, we define essential levels. You can think of the of this as for each population, where do they fit in where they start. Essential (which has reversed so 0 is the highest since version 1.x) can be thought of as the time period of start. So Essential 0 (like Defcon 1), is the most important and so forth. \n",
        "\n",
        "There are two uses of essentiality. The first is to compress very large populations into something that is understandable. If you have say 800 different classes of workers such as the US Standard Occupational Codes (SOC), then this is too much to display so you might have essential levels like healthcare worker, or blue collar worker. In the v1.x models there were seven levels:\n",
        "\n",
        "- No Protection\n",
        "- Residential\n",
        "- White collar worker (in an office setting)\n",
        "- Customer facing (in contact with many others)\n",
        "- Blue collar worker (manufacturing, construction)\n",
        "- Potential contact healthcare worker\n",
        "- In contact with COVID healthcare worker\n",
        "\n",
        "## v2.2 The other use for stage restart `Population.level_tpl` or $P^L_{tpl}\n",
        "\n",
        "This extends easily to staged restart, so for the example essentiality levels of non-healthcare employed and heathcare employed, it might look like a simple matrix across 6 start periods as or more analytically E is e rows and t columns.\n",
        "\n",
        "So in the example, it says the first population starts at time 0 and then the second starts in week 6, so in a t, e, at time zero e0 starts and then six weeks late e1 starts\n",
        "\n",
        "    1 0\n",
        "    0 0\n",
        "    0 0\n",
        "    0 0\n",
        "    0 0\n",
        "    0 0\n",
        "    0 1\n",
        "\n",
        "But this system also allows a staged restart, so for example, if you want have the workers to come back in the next period for healthcare employees and this series could even be generated as a lambda with any arbitrary function, so in this example, population 1 starts 50% in week 0, then slows continues. \n",
        "\n",
        "While population 2 doesn't start until week 4 and tails up\n",
        "\n",
        "    0.5 0\n",
        "    0.4 0\n",
        "    0.1 0\n",
        "    0 0\n",
        "    0 0.2\n",
        "    0 0.4\n",
        "    0 0.6\n",
        "\n",
        "The module has to create this. In the test model, these are hard coded, but in reality, this is tied to a government's start plan and needs to pull from a database. Create a new object for each with parameters as needed to get the right.\n",
        "\n",
        "This will let us extend the requirements out in time easily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjHZ9bRmfhL",
        "colab_type": "text"
      },
      "source": [
        "# Population to levels `Population.level` $E^R_{en}$\n",
        "\n",
        "In some sense we are doing compression by this, so we are looking at Essential index e is much less than the number of populations p. Or more succinctly e << p and we can get to E with a transpose\n",
        "\n",
        "$EP^T_{ep} * P^R{pn} = e x p * p x n = E^R{en}$\n",
        "\n",
        "    # In python this looks like\n",
        "    E_R = EP^T @ P_R\n",
        "    Ess_res_en_df = Essential_to_pop_ep_df @ Pop_res_pn_df\n",
        "\n",
        "## In v2.1 Population to essentialy over time `Ess_res_ten[t, e, n]` $E^R_{ten}$\n",
        "\n",
        "This is this we are extneding the essentiality over time by stacking\n",
        "\n",
        "    Ess_res_ten_df = Einsum(\"tn, en= ten\", Time_by_ess_tn_df, Ess_res_en_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqiejks7lR6g",
        "colab_type": "text"
      },
      "source": [
        "# Resource Class: Cost for essential levels for every item: `Resource.cost_ln_df`, $R^C_{ln}$\n",
        "\n",
        "For simplicity we can assume that each essential level has different costs. In reality, the costs will actually be more complicate and C[p, n] which is much more complicated we leave out as p can be very large and e is usually a small number of levels usual less than 10.\n",
        "\n",
        "For each item, what is the cost for N95 is $3 and non-surgical is $0.50 for high volume say healthcare workers and then higher costs for the rest of the population so there might be way s=3 scenarios\n",
        "\n",
        "    Resource.cost_ln_df[l, n] = \n",
        "              [[ $3, $0.50 ],\n",
        "               [ $4.5, $1.0]]\n",
        "\n",
        "\n",
        "### v2.1.2 Adding a r range of costs `Resource.cost_rln[r, l, n]`, $R^C_{rln)$\n",
        "Without the time components, if you add a range, you get a range of costs\n",
        "\n",
        "Later on as an example, although this should really use iterators to generate it since this is just 50% more for each\n",
        "see https://djangostars.com/blog/list-comprehensions-and-generator-expressions\n",
        "\n",
        "    $R^C_{rln} = ^{Clow}_{en}, R^{mid}_{en}, C^{hi}_{en}$\n",
        "\n",
        "    Cost_essetial_ren[r, e, n] = [[[$2, $0.25],\n",
        "                   [$3, $0.50]],\n",
        "                  [[$3, $0.50],\n",
        "                   [$4.5, $1]],\n",
        "                  [[$5, $1]\n",
        "                   [$6, $2]]]\n",
        "\n",
        "### v2.2 Adding time and the supply model 'Supply.cost_essential_tren[t,r,e,n)\n",
        "For the surge model, we just look at the peak required, but in fact we are going to get orders and then a series of prices and quantities over time t so for every time period, we get different costs for each essential level and each resource tiem\n",
        "\n",
        "    Cost_ess_res_tren[r, t, e, n] = The cost at time t for every time\n",
        "\n",
        "## Daily cost calculations $DC^E_{en}$ `Supply.daily_cost_ess_en[e, n]`\n",
        "\n",
        "OK that was the hard stuff, with these matrices reduced to essential levels and the equipment needed for each. If you now have the units needed for each essential level per period (usually day), then there are some other things you want and this is a simple scaler multiply\n",
        "\n",
        "    $DC^E_{en} = C^E_{en} * E^R_{en}$\n",
        "    Daily_cost_ess_en = Cost_essential_en_df * Ess_res_en_df.values\n",
        "\n",
        "So we have $DC^E_{en}$ which is the total units needed, now we need to do the cost analysis assuming we have variable costs\n",
        "\n",
        "### v2.1 Get a range of costs\n",
        "This is basically using casting to get Ed_{en} broadcase across all the Cost range dimension. We need to make sure this broadcast works \n",
        "\n",
        "    # Note we reverse this, assuming that broadcast has to happen\n",
        "    Daily_cost_ess_ren = np.einsum(\"ren,en=ren\", Cost_essential_ren_df * Ess_res_en_df.values\n",
        "\n",
        "### v2.2 Extent this over time\n",
        "\n",
        "    Daily_cost_ess_tren = np.einsum(\"tren,en=tren\", Cost_essential_tren_df * Ess_res_en_df.values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcQFx6VAlSX5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Model: Stockpile need by essential levels is `Resource.stockpile_en`, $S[e,n]$\n",
        "The second analysis has to do with warehouse needs and sensitivity\n",
        "\n",
        "- Stockpile estimates for say a 30-day stockpile\n",
        "- Volume estimates for daily use\n",
        "\n",
        "So for each essential you need a different stockpile. Usually more essential needs more levels. This is really another tensor.\n",
        "\n",
        "We are doing this to allow a simple Einsum to calculate all of these rather than one at a time. See the sections below for the non-tensor calculation, but the tensor one looks like\n",
        "\n",
        "    S[d, e, n ] = Einsum(\"ed, en = den\", R[e, n], Resources[n, d])\n",
        "    Stockpile needed for d Days = S[e, n] = R[e, n] * SE[e, 1].value\n",
        "\n",
        "This let's you calculate lots of parameters at the same time giving you a cost range, stockpile ranges and volumes for warehouse calculations\n",
        "\n",
        "Obviously you may not want to stock pile for all e Essential levels, so you just select what you want for instance S[0] will give you the stockpile needs for the most essential level 0.\n",
        "\n",
        "## Gross Cost for equipment by level Resource.total_cost_en\n",
        "\n",
        "So both the cross cost and the stockpile are done by level as a element-wide multiplication.\n",
        "\n",
        "    Gross cost for the equipment = RC[e, n] = R[e, n] * C[e, n].value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o_lYYWBgOsH",
        "colab_type": "text"
      },
      "source": [
        "# Detail of Model\n",
        "\n",
        "These are the details of the model. It is a good example of the parameters that you will need to add. Make sure that you have good advice from medical authorities when looking over these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU4PflJ5b2zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPZYjkTlg8CS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# https://colab.research.google.com/notebooks/forms.ipynb#scrollTo=ZCEBZPwUDGOg\n",
        "#@title Basic Model Parameters\n",
        "#@markdown ####Enter Model Description here:\n",
        "\n",
        "model_name = 'NYC Surge Forecast'  #@param\n",
        "model_description = 'v1.4 WHO Surge'  #@param {type: \"string\"}\n",
        "recurrence_index = 45  #@param {type: \"slider\", min: 0, max: 100}\n",
        "recovery_index = 62  #@param {type: \"slider\", min: 0, max: 100}\n",
        "revision =   103#@param {type: \"number\"}\n",
        "date = '2020-05-20'  #@param {type: \"date\"}\n",
        "model_type = \"surge\"  #@param ['surge', '3-month', '6-month']\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ####Daily Usage of Equipment Per Person\n",
        "units = \"10,000\" #@param [\"1,000,000\", \"100,000\", \"10,000\", \"1,000\"] {allow-input: true}\n",
        "\n",
        "#@markdown ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFSWaOVi7tR",
        "colab_type": "text"
      },
      "source": [
        "## Daily Usage of Equipment __n__ by Protection levels __l__ is D[l, n]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwe8g08yRbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47134856-1625-4e87-ccb3-c2058c17a658"
      },
      "source": [
        "# Eventually we will do this from a database import, but for now, let's use\n",
        "# the data that is normally in the Excel sheet and just recreate \n",
        "# https://colab.research.google.com/drive/1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW?authuser=5#scrollTo=1xwe8g08yRbG\n",
        "\n",
        "# Using PEP https://www.python.org/dev/peps/pep-0008/\n",
        "# For simplicity do as a dictionary\n",
        "Item_name = [\n",
        "              'N95 Surgical Respirator',\n",
        "              'N95 Mask',\n",
        "              'ASTM 3 Surgical Mask',\n",
        "              'ASTM 1-2 Surgical Mask',\n",
        "              'Non-ASTM Mask'\n",
        "              'Reusable Cotton Mask'\n",
        "              'Cotton Mask with Ear Loop',\n",
        "              'Face Shield',\n",
        "              'Goggles'\n",
        "              'Gown',\n",
        "              'Gloves',\n",
        "              'Shoe Covers',\n",
        "              'Test Kits',\n",
        "              'Disinfectant (30ml)',\n",
        "              'Disinfectant wipes'\n",
        "            ]\n",
        "\n",
        "# For this demo, we will just test with two\n",
        "Level_name = [ 'WA0', 'WA1', 'WA2', 'WA3', 'WA4', 'WA5', 'WA6']\n",
        "Item_name = [ 'N95 Surgical', 'non ASTM Mask']\n",
        "print('Item_names', Item_name)\n",
        "Daily_usage_matrix = [\n",
        "                [ 0, 0 ],\n",
        "                [ 0, 1 ],\n",
        "                [ 0, 2 ],\n",
        "                [ 0.1, 3],\n",
        "                [ 0.2, 4],\n",
        "                [ 0.3, 6],\n",
        "                [ 1.18, 0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Item_names ['N95 Surgical', 'non ASTM Mask']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFHD-0I8JU8N",
        "colab_type": "text"
      },
      "source": [
        "### Daily Usage Matrix verification and conversion to Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uM7R8IXJTQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d653e1ee-81b9-4ee2-dd12-569e266389ab"
      },
      "source": [
        "print('Daily_usage_matrix', Daily_usage_matrix)\n",
        "\n",
        "Daily_usage_df = pd.DataFrame(Daily_usage_matrix,\n",
        "                              columns = Item_name,\n",
        "                              index = Level_name)\n",
        "\n",
        "# use these counts to check the matrix vector bugs\n",
        "level_count = Daily_usage_df.shape[0]\n",
        "item_count = Daily_usage_df.shape[1]\n",
        "print('usage_pd shape is ', Daily_usage_df.shape,\n",
        "      'protection level count is ', level_count,\n",
        "      'item count is ', item_count)\n",
        "\n",
        "print('Daily_usage_pd', Daily_usage_df)\n",
        "\n",
        "Daily_N95s_usage = Daily_usage_df['N95 Surgical']\n",
        "print('Daily N95 Surgical Usage', Daily_N95s_usage)\n",
        "\n",
        "# https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
        "print('Daily usage value in Dataframe', Daily_usage_df.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Daily_usage_matrix [[0, 0], [0, 1], [0, 2], [0.1, 3], [0.2, 4], [0.3, 6], [1.18, 0]]\n",
            "usage_pd shape is  (7, 2) protection level count is  7 item count is  2\n",
            "Daily_usage_pd      N95 Surgical  non ASTM Mask\n",
            "WA0          0.00              0\n",
            "WA1          0.00              1\n",
            "WA2          0.00              2\n",
            "WA3          0.10              3\n",
            "WA4          0.20              4\n",
            "WA5          0.30              6\n",
            "WA6          1.18              0\n",
            "Daily N95 Surgical Usage WA0    0.00\n",
            "WA1    0.00\n",
            "WA2    0.00\n",
            "WA3    0.10\n",
            "WA4    0.20\n",
            "WA5    0.30\n",
            "WA6    1.18\n",
            "Name: N95 Surgical, dtype: float64\n",
            "Daily usage value in Dataframe [[0.   0.  ]\n",
            " [0.   1.  ]\n",
            " [0.   2.  ]\n",
            " [0.1  3.  ]\n",
            " [0.2  4.  ]\n",
            " [0.3  6.  ]\n",
            " [1.18 0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4uMRbROcqpf",
        "colab_type": "text"
      },
      "source": [
        "## Population Data by sub-populations p is P[p, 1]\n",
        "\n",
        "Start with the simplest assumption, two populations, one that is `WA6` and one that is `WA2` as an example. But we will insert more data later once we decide the data source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odyDmPbkc3l2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9c5d7952-b19e-4f07-ac49-85e383bdb0db"
      },
      "source": [
        "# This is a dummy test case, later we will use extraction first form a\n",
        "# spreadsheet and then eventually from a data store that is reliable\n",
        "# And which has revision control\n",
        "\n",
        "class Population:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "Population_name = ['Healthcare employees', 'Non employees of healthcare companies']\n",
        "Population_data = [735.2, 7179.6]\n",
        "\n",
        "print('Population Data', Population_data)\n",
        "\n",
        "Population_df = pd.DataFrame(Population_data, index = Population_name, columns = ['Population'])\n",
        "population_count = Population_df.shape[0]\n",
        "print('population count p', population_count)\n",
        "print(Population_df)\n",
        "\n",
        "# https://note.nkmk.me/en/python-type-isinstance/\n",
        "print('type of Population_name', type(Population_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Population Data [735.2, 7179.6]\n",
            "population count p 2\n",
            "                                       Population\n",
            "Healthcare employees                        735.2\n",
            "Non employees of healthcare companies      7179.6\n",
            "type of Population_name <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqG2MmDelJZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOgyfxTet7N",
        "colab_type": "text"
      },
      "source": [
        "# Usage of PPE by Sub-population p is U[p, l]\n",
        "\n",
        "Now we have a vector which are the population usages and we have a list of needs, so we need to do a matrix multiply of population by needs. Each entry is the percentage of a population at a given level.\n",
        "\n",
        "So in this example, 50% of healthcare workers are level 5 and 50% are at level 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA65fr95e8w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5287337a-98db-4804-ab76-658add41c574"
      },
      "source": [
        "# Now we need a matrix which is the pop_type x usage_type and the coefficient is just how much is needed for each\n",
        "# Do this for simplicity start with a zero matrix, we will actually load the data\n",
        "\n",
        "Usage_by_population_matrix = np.zeros([population_count, level_count])\n",
        "\n",
        "Usage_by_population_matrix[1,1] = 1.0\n",
        "Usage_by_population_matrix[0,6] = Usage_by_population_matrix[0, 5] = 0.5\n",
        "print('Usage_by_population_matrix', Usage_by_population_matrix)\n",
        "\n",
        "# https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/\n",
        "Usage_by_population_df = pd.DataFrame(Usage_by_population_matrix,\n",
        "                                      index = Population_name,\n",
        "                                      columns = Level_name)\n",
        "print(Usage_by_population_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage_by_population_matrix [[0.  0.  0.  0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0.  0.  0.  0. ]]\n",
            "                                       WA0  WA1  WA2  WA3  WA4  WA5  WA6\n",
            "Healthcare employees                   0.0  0.0  0.0  0.0  0.0  0.5  0.5\n",
            "Non employees of healthcare companies  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGfH93RCkjXk",
        "colab_type": "text"
      },
      "source": [
        "# Required Equipment n per capita per sub-population p per capita is R[p, n]\n",
        "\n",
        "This is the first multiplication where we take the two matrices and multiply them together. So this will give us a matrix. Each row is for the populations and then each column shows the daily usage by population."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUb-b6EPlUre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8e7a46e6-c0b5-4c01-ba0e-2ae48f80add2"
      },
      "source": [
        "print('Daily_usage_df', Daily_usage_df.shape)\n",
        "print('Usage_by_population_df', Usage_by_population_df.shape)\n",
        "\n",
        "# Note with Panda multiply the index of rows and the columns have to match\n",
        "Required_df = Usage_by_population_df @ Daily_usage_df\n",
        "\n",
        "print('Required_df', Required_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Daily_usage_df (7, 2)\n",
            "Usage_by_population_df (2, 7)\n",
            "Required_df                                        N95 Surgical  non ASTM Mask\n",
            "Healthcare employees                           0.74            3.0\n",
            "Non employees of healthcare companies          0.00            1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmHR1tXJMYW0",
        "colab_type": "text"
      },
      "source": [
        "# Total Required equipment for each Population T[p, n]\n",
        "\n",
        "We are now just going to case the Population count vector across the required per capita to get the total required across all populations. So we need [element-wise multiplication](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy) which is denoted and this works because of casting, so P is duplicated for each column. Th syntax is different in each variant, for [Dataframes](https:/stackoverflow.com/questions/21022865/pandas-elementwise-multiplication-of-two-dataframes_)\n",
        "\n",
        "    # In Matlab\n",
        "    R .* P \n",
        "    # In Numpy\n",
        "    R * P\n",
        "    # In \n",
        "    R * P.values\n",
        "\n",
        "This is a pretty easy calculation, you just need the element-wise multiplication of the actual population numbers against the per-capita needs. Because of t he way broadcasting works, the vector is spread properly\n",
        "\n",
        "$T[p, n] = R[p, n] * P[p, 1].values$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZn5jauDNHgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d2451eeb-a732-48a0-ab49-3b047ec633f5"
      },
      "source": [
        "\n",
        "print('Required_df shape', Required_df.shape)\n",
        "print(Required_df)\n",
        "print('Population_df shape', Population_df.shape)\n",
        "print(Population_df)\n",
        "\n",
        "Total_required_df = Required_df * Population_df.values\n",
        "print(Total_required_df)\n",
        "\n",
        "# another formulation\n",
        "Total_items_per_population_df = Required_df * Population_df.values\n",
        "print('Total items for each subpopulation')\n",
        "print(Total_items_per_population_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Required_df shape (2, 2)\n",
            "                                       N95 Surgical  non ASTM Mask\n",
            "Healthcare employees                           0.74            3.0\n",
            "Non employees of healthcare companies          0.00            1.0\n",
            "Population_df shape (2, 1)\n",
            "                                       Population\n",
            "Healthcare employees                        735.2\n",
            "Non employees of healthcare companies      7179.6\n",
            "                                       N95 Surgical  non ASTM Mask\n",
            "Healthcare employees                        544.048         2205.6\n",
            "Non employees of healthcare companies         0.000         7179.6\n",
            "Total items for each subpopulation\n",
            "                                       N95 Surgical  non ASTM Mask\n",
            "Healthcare employees                        544.048         2205.6\n",
            "Non employees of healthcare companies         0.000         7179.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjU4CuZGYi_w",
        "colab_type": "text"
      },
      "source": [
        "# Now convert Population rows into Essential rows with E[e, p]\n",
        "This changes the labels and let's you assign each Population with an Essentiality index. Eventually, the essentiality will represent a time series. So e=0 means start at week 0 (for healthcare) and then e=N means start at week N. \n",
        "\n",
        "We can even do a time series on that too, say have a sigmoid for the starting or some other sort of lambda.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvE_lclnbL4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0bb1232d-255c-4c60-f20b-8eef413cfb90"
      },
      "source": [
        "# this is a square inverse as we healthcare works are week 0 and non-healthcare is week 1\n",
        "\n",
        "Essential_name = [ \"Essential\",\n",
        "                   \"Non-essential\"]\n",
        "\n",
        "Essential_by_population_matrix = [\n",
        "                                    [1, 0],\n",
        "                                    [0, 1]                                 \n",
        "                                  ]\n",
        "\n",
        "Essential_by_population_df = pd.DataFrame(Essential_by_population_matrix,\n",
        "                                      index = Essential_name,\n",
        "                                      columns = Population_name)\n",
        "\n",
        "print('Essential by population', Essential_by_population_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Essential by population                Healthcare employees  Non employees of healthcare companies\n",
            "Essential                         1                                      0\n",
            "Non-essential                     0                                      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HzfZeTtbRFY",
        "colab_type": "text"
      },
      "source": [
        "# Now use that matrix to convert Required equipment by population to Required by Essentiality\n",
        "\n",
        "So we that T[p, n] and we convert with another matrix multiple as\n",
        "\n",
        "    RE[e, n] = E[e, p] @ T[p, n] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX2LPIO2clDN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9af678d9-f97d-4c6b-c6ba-1bac0aa7f116"
      },
      "source": [
        "Required_by_essential_df = Essential_by_population_df @ Total_required_df\n",
        "print('Require by Essential Index', Required_by_essential_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Require by Essential Index                N95 Surgical  non ASTM Mask\n",
            "Essential           544.048         2205.6\n",
            "Non-essential         0.000         7179.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hj_MLrCc7zZ",
        "colab_type": "text"
      },
      "source": [
        "# Cost matrix is the Cost per item for all items C[e, n]\n",
        "\n",
        "For each row, we get a cost for the item, so for intance, if N95 surgicals are $3 and non-ASTM disposables are $0., the the vector looks like, because of broadcasting, if you just define a single row, it will be copied against all esesential levels\n",
        "\n",
        "    CE[e, n] =[ $3.00, $0.50]\n",
        "\n",
        "This should be the same width as the Product slicing.\n",
        "\n",
        "In this simple case, all costs across all essentials are identical, but in the more sophisticated costs, costs will vary by volume, so cost becomes a different across different essential levels as volumes and purchasing power are different\n",
        "\n",
        "    CE[e, n] = [ [ $3, $0.20 ],\n",
        "                [ $5, $0.50 ]\n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qpsY-HvdGDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "94219115-44f0-404f-97ed-d5a132493696"
      },
      "source": [
        "# dataframe is a matrix\n",
        "# Assumes the same price for all users\n",
        "# Note that to make the math work, it need to a Numpy Array\n",
        "Cost_per_item_by_essential_matrix = np.array([ 3, 0.5] )\n",
        "\n",
        "# Assumes a different price depending on the essnetial level is 50% more\n",
        "Cost_per_item_by_essential_matrix = [ Cost_per_item_by_essential_matrix, Cost_per_item_by_essential_matrix * 1.5 ]\n",
        "Cost_per_item_by_essential_df = pd.DataFrame(Cost_per_item_by_essential_matrix,\n",
        "                                index = Essential_name,\n",
        "                                columns = Item_name)\n",
        "print('Cost per item',Cost_per_item_by_essential_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost per item                N95 Surgical  non ASTM Mask\n",
            "Essential               3.0           0.50\n",
            "Non-essential           4.5           0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUF3Zt2imJiA",
        "colab_type": "text"
      },
      "source": [
        "# Now calculate the costs based on Requirements and Cost matrix\n",
        "\n",
        "this is just the element-wise multiplication\n",
        "\n",
        "TE[e, n] = RE[e, n] * CE[e, n].value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A-BZeN-pJ-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "78ccf7cc-6ab2-45b1-e594-023c5f8bcdec"
      },
      "source": [
        "Total_cost_by_essential_df = Required_by_essential_df * Cost_per_item_by_essential_df.values\n",
        "print('Total cost by essential_df', Total_cost_by_essential_df )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total cost by essential_df                N95 Surgical  non ASTM Mask\n",
            "Essential          1632.144         1102.8\n",
            "Non-essential         0.000         5384.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dNjaRk-p87e",
        "colab_type": "text"
      },
      "source": [
        "# Finally use the Stockpile in days per essential to get the Stockpile by essential population\n",
        "\n",
        "This is another element wise multiply\n",
        "\n",
        "S[e, n] = RE[e, n] * DE[1, n].values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0BcGolJqS83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7835d1b9-1c2c-4a08-db9d-dab4fb046f24"
      },
      "source": [
        "# how much stockpile per item is needed for each level\n",
        "Day_stockpile_by_essential_matrix = [ [30], \n",
        "                                     [0]]\n",
        "Day_stockpile_by_essential_df = pd.DataFrame(Day_stockpile_by_essential_matrix,\n",
        "                                             index= Essential_name)\n",
        "\n",
        "print('Day_stockpile_by essential', Day_stockpile_by_essential_df,)\n",
        "\n",
        "# use .to_numpy as clearer than .values but this does not work\n",
        "Stockpile_by_essential_df = Required_by_essential_df * Day_stockpile_by_essential_df.values\n",
        "\n",
        "print('Stockpile by essential', Stockpile_by_essential_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Day_stockpile_by essential                 0\n",
            "Essential      30\n",
            "Non-essential   0\n",
            "Stockpile by essential                N95 Surgical  non ASTM Mask\n",
            "Essential          16321.44        66168.0\n",
            "Non-essential          0.00            0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6tdby_AHdre",
        "colab_type": "text"
      },
      "source": [
        "# Estimating the Behavior over time BM[p, t] by population\n",
        "So this is a flatten matrix for each population, what happens over time. So for instance 1.0 means the same activity level as pre-COVID-19. You can have more than 1.0 if there is a frenzy of activity as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QuIZ5y6yesE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "60b07649-71b6-4f50-d5b8-03b622c11e2c"
      },
      "source": [
        "# An example of a hard coded matrix without parameters\n",
        "Behavioral_time_by_population_matrix = [ [ 0.5, 1, 1],\n",
        "                                         [ 0, 0, 1]]\n",
        "Period_name = [ 'Week 1', 'Week 2', 'Week 3']\n",
        "Behavioral_time_by_population_df = pd.DataFrame(Behavioral_time_by_population_matrix,\n",
        "                                                index=Population_name,\n",
        "                                                columns=Period_name)\n",
        "print('Behavioral Time Parms')\n",
        "print(Behavioral_time_by_population_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Behavioral Time Parms\n",
            "                                       Week 1  Week 2  Week 3\n",
            "Healthcare employees                      0.5       1       1\n",
            "Non employees of healthcare companies     0.0       0       1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQPcQVkTVGDd",
        "colab_type": "text"
      },
      "source": [
        "# A matrix approach to stockpile\n",
        "Instead of separate calculations for each we create a details for the Resource is a vector Rd[unit, stockpile-30, stockpile-60, stockpile-90] In this way with a single multiple of Rd[p, d]\n",
        "\n",
        "So to calculate the \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhJXjmqHzBkk",
        "colab_type": "text"
      },
      "source": [
        "# Creating a time series for Resources needed R[p, n] from BM[p, t] to Rtime[p, n, t]\n",
        "\n",
        "You can take any matrix and make it time oriented. So let's take the essential Stockpile and vary it by time.\n",
        "\n",
        "What is easiest to do with the time series feature of is to flatten all the matrices. And then it seems you just need to add a timestamp.\n",
        "\n",
        "The other apporach is to use the so called multiindex which let's you do multi dimensionals in a 2-D way.\n",
        "\n",
        "The use of iterables makes it easy to mix the labels in [multiindexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) but this is really a display thing. You cannot easily take a multidimensional cube and do a multiply. so the plan is to convert the multi-index to a numpy tensor do the math and then put it back.\n",
        "\n",
        "But the easier way is to do this as a pure matrix with numpy thanks to [Stackoverflow](https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays) the key is something called einsum which does give you a simple way to transform\n",
        "\n",
        "    np.einsum('ij, jk -> ijk', A, B)\n",
        "    # this basically extends the A[i,j] into the k dimension which is what we want where i=p, j=n, k=t\n",
        "    # these need to be in alphabetical order \n",
        "    np.einsum('ij,ik -> ijk ', R, BM)\n",
        "\n",
        "The second problem is how to make a multi-dimensional matrix (a tensor) displayable with Pandas, the answer seems to be a [multiIndex](https://stackoverflow.com/questions/43921419/transforming-multiindex-to-row-wise-multi-dimensional-numpy-array) which you create by cleaver reshaping, that is a multi-dimensional is basically a tensor unwrapped to a single vector, so you need to reshape it for a 2-D, it looks like\n",
        "\n",
        "    m,n = len(df.index.levels[0]), len(df.index.levels[1])\n",
        "    arr = df.values.reshape(m,n,-1).swapaxes(1,2)\n",
        "\n",
        "You can see how this becomes completely general as pseudocode where you get the lenght of each index and then reshape it\n",
        "\n",
        "    dimensions = len(df.index.levels)\n",
        "    tensor = df.values.reshape(dimensions,-1).swapaxes(reverse(dimensions))\n",
        "\n",
        "Or even more elegant [stackoverflow](https://stackoverflow.com/questions/35047882/transform-pandas-dataframe-with-n-level-hierarchical-index-into-n-d-numpy-array) points out that if you just create the right shaped numpy array, you can pour it in with flatten and this is real code, note the cool use of the map function\n",
        "\n",
        "    # create an empty array of NaN of the right dimensions\n",
        "    shape = map(len, df.index.levels)\n",
        "    array = np.full(shape, np.nan)\n",
        "    # fill it using Numpy's advanced indexing\n",
        "    array[df.index.labels] = df.values.flat\n",
        "\n",
        "The conversion from numpy array (tensor) to Pandas MultiIndex [stackoverflow](https://stackoverflow.com/questions/43427189/3-dimensional-numpy-array-to-multiindex-pandas-dataframe/48755377) uses the Panel\n",
        "\n",
        "    df = pd.Panel(arr).to_frame()\n",
        "\n",
        "But this is [deprecated](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Panel.html) since a panel only works for 3 dimensionals, but we can use this just to do the conversion easily.\n",
        "\n",
        "The basic strategy seems to be to flatten the numpy array and then create the index as the unrolled names. \n",
        "\n",
        "    df = pd.Series(arr, index=unrolled names)\n",
        "\n",
        "[Stackoverflow](https://riptutorial.com/pandas/example/6439/create-a-sample-dataframe-with-multiindex) explains that multiindex is really just a different index and you just stick it onto a vectors, so the idea is to take the first N-1 dimensions and make them a multi-index and then the last index (the column) can still sit there.\n",
        "\n",
        "    # you can create the index just by giving the labels in two sets\n",
        "    idx = pd.MultiIndex.from_product([['bar', 'baz', 'foo'],['one','two']])\n",
        "    # Then you can have columns which are the data itself\n",
        "    df = pd.DataFrame(np.random.randn(8, 2), index=idx, columns=['A', 'B'])\n",
        "\n",
        "The final piece of the puzzle is using [Pandas Time Series](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html) so you can utter things like this to create an index on time\n",
        "\n",
        "    index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
        "                          '2015-07-04', '2015-08-04'])\n",
        "    data = pd.Series([0, 1, 2, 3], index=index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DThfVZtG4Z7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d02d440a-89fe-407e-994d-01f2cdb2c4ae"
      },
      "source": [
        "# https://stackoverflow.com/questions/25440008/python-pandas-flatten-a-dataframe-to-a-list\n",
        "# Not the right approach what we want is a stack of R[p, n]\n",
        "# When R[p, n] * BM[p, 1] => Rt[p, n .* BM[p,1]]\n",
        "R = np.array([[ 0, 100],\n",
        "              [50, 0]])\n",
        "print('Test R', R)\n",
        "BM = np.array ([ [ 0, 1],\n",
        "                 [ 1, 1]])\n",
        "print('Test BM', BM)\n",
        "Rt=np.zeros((2,2,2))\n",
        "Rt1=np.zeros((2,2))\n",
        "print('Rt', Rt)\n",
        "# https://stackoverflow.com/questions/4455076/how-to-access-the-ith-column-of-a-numpy-multidimensional-array\n",
        "# get the column vector for time 0 note that [0] means return a column vector not a row vector\n",
        "BM0=BM[:,[0]]\n",
        "print('BM0', BM0)\n",
        "print('BM0.shape', BM0.shape)\n",
        "\n",
        "# This should broadcast so R is p xn and BM0 is p x 1\n",
        "Rt0 = R * BM0\n",
        "print('Rt[:,:,0]', Rt0)\n",
        "\n",
        "print('Total Resource by population rp x n')\n",
        "print(Total_required_df)\n",
        "print('shape', Total_required_df.shape)\n",
        "\n",
        "print('Behavorial model p x t')\n",
        "print(Behavioral_time_by_population_df)\n",
        "print('shape', Behavioral_time_by_population_df.shape)\n",
        "\n",
        "# Now the magic note the output is in alphabetical order\n",
        "# https://ajcr.net/Basic-guide-to-einsum/\n",
        "# Total_required_df i rows, j columns; Behavorial_time i row, k column\n",
        "ijk=np.einsum('ij, ik -> ijk', Total_required_df, Behavioral_time_by_population_df)\n",
        "print('ijk', ijk.shape, ijk)\n",
        "\n",
        "kji=np.einsum('ij, ik -> kji', Total_required_df, Behavioral_time_by_population_df)\n",
        "print('kji', kji.shape, kji)\n",
        "print('kji[0]', kji[0])\n",
        "\n",
        "kij=np.einsum('ij, ik -> kij', Total_required_df, Behavioral_time_by_population_df)\n",
        "print('kij', kij.shape, kij)\n",
        "print('kij[0]', kij[0])\n",
        "\n",
        "# p = population, n = items, t = time\n",
        "Total_required_by_time_matrix = np.einsum('pn, pt -> tpn', Total_required_df, Behavioral_time_by_population_df)\n",
        "print('Total required over time')\n",
        "print(Total_required_by_time_matrix)\n",
        "\n",
        "# Test conversion to a Pandas MultiIndex\n",
        "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
        "# you can create the index just by giving the labels in two sets\n",
        "idx = pd.MultiIndex.from_product([['bar', 'baz', 'foo', 'fob'],['one','two']])\n",
        "print('idx', idx)\n",
        "# Then you can have columns which are the data itself\n",
        "df = pd.DataFrame(np.random.randn(8, 2), index=idx, columns=['A', 'B'])\n",
        "print('df', df)\n",
        "\n",
        "# first create dates\n",
        "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\n",
        "Time_name = pd.date_range(start='6/10/2020', periods=3, freq='W')\n",
        "print('Time', Time_name)\n",
        "print('Population names', type(Population_name), Population_name)\n",
        "Total_required_by_time_index = pd.MultiIndex.from_product([Time_name, Population_name]) \n",
        "print('Indices', Total_required_by_time_index)\n",
        "\n",
        "# flatten out all but the last index\n",
        "last_dim = Total_required_by_time_matrix.shape[-1]\n",
        "print('size of the last dimension', last_dim)\n",
        "print('size of the time total', Total_required_by_time_matrix.size)\n",
        "# Reshape the matrix to be 2D, not you need to force the division to be an\n",
        "# integer\n",
        "Total_required_by_time_2d = Total_required_by_time_matrix.reshape(\n",
        "    (int(Total_required_by_time_matrix.size/last_dim),\n",
        "    last_dim))\n",
        "\n",
        "# Note you cannot pass 3D matrices, so flatten it into 2D\n",
        "Total_required_by_time_df = pd.DataFrame(Total_required_by_time_2d,\n",
        "                                         index=Total_required_by_time_index,\n",
        "                                         columns=Item_name)\n",
        "\n",
        "print('Totel', Total_required_by_time_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test R [[  0 100]\n",
            " [ 50   0]]\n",
            "Test BM [[0 1]\n",
            " [1 1]]\n",
            "Rt [[[0. 0.]\n",
            "  [0. 0.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]]\n",
            "BM0 [[0]\n",
            " [1]]\n",
            "BM0.shape (2, 1)\n",
            "Rt[:,:,0] [[ 0  0]\n",
            " [50  0]]\n",
            "Total Resource by population rp x n\n",
            "                                       N95 Surgical  non ASTM Mask\n",
            "Healthcare employees                        544.048         2205.6\n",
            "Non employees of healthcare companies         0.000         7179.6\n",
            "shape (2, 2)\n",
            "Behavorial model p x t\n",
            "                                       Week 1  Week 2  Week 3\n",
            "Healthcare employees                      0.5       1       1\n",
            "Non employees of healthcare companies     0.0       0       1\n",
            "shape (2, 3)\n",
            "ijk (2, 2, 3) [[[ 272.024  544.048  544.048]\n",
            "  [1102.8   2205.6   2205.6  ]]\n",
            "\n",
            " [[   0.       0.       0.   ]\n",
            "  [   0.       0.    7179.6  ]]]\n",
            "kji (3, 2, 2) [[[ 272.024    0.   ]\n",
            "  [1102.8      0.   ]]\n",
            "\n",
            " [[ 544.048    0.   ]\n",
            "  [2205.6      0.   ]]\n",
            "\n",
            " [[ 544.048    0.   ]\n",
            "  [2205.6   7179.6  ]]]\n",
            "kji[0] [[ 272.024    0.   ]\n",
            " [1102.8      0.   ]]\n",
            "kij (3, 2, 2) [[[ 272.024 1102.8  ]\n",
            "  [   0.       0.   ]]\n",
            "\n",
            " [[ 544.048 2205.6  ]\n",
            "  [   0.       0.   ]]\n",
            "\n",
            " [[ 544.048 2205.6  ]\n",
            "  [   0.    7179.6  ]]]\n",
            "kij[0] [[ 272.024 1102.8  ]\n",
            " [   0.       0.   ]]\n",
            "Total required over time\n",
            "[[[ 272.024 1102.8  ]\n",
            "  [   0.       0.   ]]\n",
            "\n",
            " [[ 544.048 2205.6  ]\n",
            "  [   0.       0.   ]]\n",
            "\n",
            " [[ 544.048 2205.6  ]\n",
            "  [   0.    7179.6  ]]]\n",
            "idx MultiIndex([('bar', 'one'),\n",
            "            ('bar', 'two'),\n",
            "            ('baz', 'one'),\n",
            "            ('baz', 'two'),\n",
            "            ('foo', 'one'),\n",
            "            ('foo', 'two'),\n",
            "            ('fob', 'one'),\n",
            "            ('fob', 'two')],\n",
            "           )\n",
            "df                 A         B\n",
            "bar one  0.980999 -0.568487\n",
            "    two -2.150622  0.023941\n",
            "baz one -0.220107 -1.415076\n",
            "    two -0.465516  0.594853\n",
            "foo one -0.199059 -0.077765\n",
            "    two  0.584756 -0.399014\n",
            "fob one  0.527362  1.866949\n",
            "    two -1.619264  1.012010\n",
            "Time DatetimeIndex(['2020-06-14', '2020-06-21', '2020-06-28'], dtype='datetime64[ns]', freq='W-SUN')\n",
            "Population names <class 'list'> ['Healthcare employees', 'Non employees of healthcare companies']\n",
            "Indices MultiIndex([('2020-06-14',                  'Healthcare employees'),\n",
            "            ('2020-06-14', 'Non employees of healthcare companies'),\n",
            "            ('2020-06-21',                  'Healthcare employees'),\n",
            "            ('2020-06-21', 'Non employees of healthcare companies'),\n",
            "            ('2020-06-28',                  'Healthcare employees'),\n",
            "            ('2020-06-28', 'Non employees of healthcare companies')],\n",
            "           )\n",
            "size of the last dimension 2\n",
            "size of the time total 12\n",
            "Totel                                                   N95 Surgical  non ASTM Mask\n",
            "2020-06-14 Healthcare employees                        272.024         1102.8\n",
            "           Non employees of healthcare companies         0.000            0.0\n",
            "2020-06-21 Healthcare employees                        544.048         2205.6\n",
            "           Non employees of healthcare companies         0.000            0.0\n",
            "2020-06-28 Healthcare employees                        544.048         2205.6\n",
            "           Non employees of healthcare companies         0.000         7179.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmzf8K60yA7a",
        "colab_type": "text"
      },
      "source": [
        "# Behavioral modeling BM[p, t] from parameters BMp[p, 2]\n",
        "This long term will be a Python function from a PIP library which will also get instantiated as a REST API so it can be called. (see the section on implementation\n",
        "\n",
        "But this implements the time series of restart across a set of populations. The interface is the Dataframe BM[p, t] which is a factor for how much activitiy there is the Population matrix. So this model says the first population, the non-healthcare workers start 40% in week 2 and then get to 60% by week 3.\n",
        "\n",
        "The healthcare workers start and this is manually encoded.\n",
        "\n",
        "For simplicity, you can use a generator function as well to create this so you don't have to create a gigantic table by hand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7NW8qt4SN6U",
        "colab_type": "text"
      },
      "source": [
        "## Using parameters to create the activity matrix\n",
        "This is harder than it looks, but the idea is to use some simple functions and generate a matrix. The main problem here is that you want to do it quickly, so ideally, you would use vector match to create all the matrices automatically as a template\n",
        "\n",
        "The first attempt with numpy.fromfunction sort of works, but you cannot parameterize them very well, so the solution is to use vectorize as a way to get it across an entrie array.\n",
        "\n",
        "But since this varries by row, instead, we use from function for each row and then can concatenate them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye7uD8BvSWUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88611bc0-9645-466f-d010-4eb95c03be43"
      },
      "source": [
        "# How to create an model automagically with just paramaters\n",
        "# Remember Python doesn't have arrays, it uses lists for that\n",
        "# https://www.programiz.com/python-programming/matrix\n",
        "# And index syntax is completely different from numpy\n",
        "Behavioral_by_population_parameter_matrix = [ ['logistic', 1, 0.7],\n",
        "                                              ['logistic', 0, 1 ] ]\n",
        "Logistic_parameter_name = [ 'function', 'loc', 'scale']\n",
        "\n",
        "Behavioral_by_population_parameter_df = pd.DataFrame(Behavioral_by_population_parameter_matrix,\n",
        "                                                     index=Population_name,\n",
        "                                                     columns=Logistic_parameter_name)\n",
        "\n",
        "print(Behavioral_by_population_parameter_df)\n",
        "print('scipy stats')\n",
        "from scipy.stats import logistic \n",
        "for t in range(1,3):\n",
        "  print(t, logistic.cdf(t, loc=1.4, scale=2.0))\n",
        "\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
        "# Can use a function to create a long list\n",
        "print(type(Behavioral_by_population_parameter_matrix))\n",
        "print(Behavioral_by_population_parameter_matrix)\n",
        "\n",
        "# remember Python matrices index from 0\n",
        "print(Behavioral_by_population_parameter_matrix[0][1],\n",
        "      type(Behavioral_by_population_parameter_matrix[0][2]))\n",
        "\n",
        "# https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types\n",
        "# need to turn them into real numbers\n",
        "value=Behavioral_by_population_parameter_matrix\n",
        "\n",
        "# Note that From function is not called once per cell, it is called exactly once and not once per cell\n",
        "# You need to use np.vectorize to push the function across all cell elements\n",
        "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
        "\n",
        "# https://stackoverflow.com/questions/50997928/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-1d/50997969\n",
        "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
        "                                           loc=1.2,\n",
        "                                           scale=2.3\n",
        "                                           ), (2, 3)))\n",
        "\n",
        "# This works, so you can do a lookup from a global variable inside a lambda\n",
        "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
        "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
        "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
        "                                           ), (2, 3)))\n",
        "\n",
        "\n",
        "# now try using p and this works fine\n",
        "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
        "                                           loc=p,\n",
        "                                           scale=p+1\n",
        "                                           ), (2, 3)))\n",
        "\n",
        "# This does not work, the error is only integer scalar arrays can be converted to a scalar index \n",
        "print('getting just p+t')\n",
        "# This works because you are actually doing array match, p is (2,3) and t is (2, 3)\n",
        "print(np.fromfunction(lambda p, t : p+t, (2, 3)))\n",
        "print('value of p and t')\n",
        "print(np.fromfunction(lambda p, t : p, (2, 3)))\n",
        "print(np.fromfunction(lambda p, t : t, (2, 3)))\n",
        "\n",
        "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
        "# Explains what is going on \n",
        "\n",
        "print('type of p and t')\n",
        "# Unintuitive, what you get is not a scalar in p, but a numpy array when in the above you\n",
        "# just get a scalar, why is this?\n",
        "# The function actually gets an input that is the shape noted so it is (2,3)\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
        "print(np.fromfunction(lambda p, t : type(p), (2, 3)))\n",
        "print(np.fromfunction(lambda p, t : type(t), (2, 3)))\n",
        "\n",
        "# This demonstrates that what you get is every possible point\n",
        "# so p will be all the row numbers which are the same across all the columns\n",
        "# and t will be the column indices which are the same across all columns\n",
        "# which is not what stackoverflow said\n",
        "# So to use this, you need a function which does array math properly\n",
        "def show(p, t):\n",
        "  print('show')\n",
        "  print('called value of p')\n",
        "  print(type(p), p.shape, p)\n",
        "\n",
        "  print('called value of t')\n",
        "  print(type(t), t.shape, t)\n",
        "\n",
        "print(np.fromfunction(show, (3, 3)))\n",
        "\n",
        "print('shapes of p and t')\n",
        "print(np.fromfunction(lambda p, t : p.shape, (2, 3)))\n",
        "print(np.fromfunction(lambda p, t : t.shape, (2, 3)))\n",
        "\n",
        "# This works because logistics handles arrays, so takes all of t and applies it\n",
        "# The problem is that loc and scale need scalars, so you can index it with p, which is a matrix\n",
        "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
        "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
        "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
        "                                           ), (2, 3)))\n",
        "\n",
        "# This will not work because you are getting p as an array and t as an array and it expects\n",
        "# a array function\n",
        "# print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
        "#                                           loc=Behavioral_by_population_parameter_matrix[p][1],\n",
        "#                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
        "#                                           ), (2, 3)))\n",
        "\n",
        "# The right approach is to just use the working code above but apply it to each row and then glom the whole thing together\n",
        "# https://www.pluralsight.com/guides/numpy-arrays-iterating\n",
        "# This works because numpy treats a matrix actually as a collection of lists so very elegant\n",
        "print(Behavioral_by_population_parameter_df)\n",
        "print(Behavioral_by_population_parameter_matrix)\n",
        "\n",
        "# Iterating over a 2-D dataframe which is much more complicated than a matrix\n",
        "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
        "# https://www.geeksforgeeks.org/different-ways-to-iterate-over-rows-in-pandas-dataframe/\n",
        "# note that in Dataframes, the rows are actually behind, so it is column major\n",
        "print('iterate over rows in dataframe')\n",
        "for population in Behavioral_by_population_df.index:\n",
        "  print('population', type(population), population)\n",
        "  print(Behavioral_by_population_df['Week 1'][population])\n",
        "\n",
        "print('iterate using iterrows')\n",
        "for index, population in Behavioral_by_population_df.iterrows():\n",
        "  print(index, type(population), population['Week 1'])\n",
        "\n",
        "print('iterate of rows in numpy matrix')\n",
        "for population in Behavioral_by_population_matrix:\n",
        "  # you will get each item overall\n",
        "  print(population)\n",
        "\n",
        "# Another approach is apply_along_axis which is really cool\n",
        "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
        "# \n",
        "\n",
        "# https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays\n",
        "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.outer.html\n",
        "# https://stackoverflow.com/questions/26089893/understanding-numpys-einsum\n",
        "# A completely different approach\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                       function  loc  scale\n",
            "Healthcare employees                   logistic    1    0.7\n",
            "Non employees of healthcare companies  logistic    0    1.0\n",
            "scipy stats\n",
            "1 0.45016600268752216\n",
            "2 0.574442516811659\n",
            "<class 'list'>\n",
            "[['logistic', 1, 0.7], ['logistic', 0, 1]]\n",
            "1 <class 'float'>\n",
            "[[0.37244566 0.47827456 0.58609031]\n",
            " [0.37244566 0.47827456 0.58609031]]\n",
            "[[0.19332137 0.5        0.80667863]\n",
            " [0.19332137 0.5        0.80667863]]\n",
            "[[0.5        0.73105858 0.88079708]\n",
            " [0.37754067 0.5        0.62245933]]\n",
            "getting just p+t\n",
            "[[0. 1. 2.]\n",
            " [1. 2. 3.]]\n",
            "value of p and t\n",
            "[[0. 0. 0.]\n",
            " [1. 1. 1.]]\n",
            "[[0. 1. 2.]\n",
            " [0. 1. 2.]]\n",
            "type of p and t\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "show\n",
            "called value of p\n",
            "<class 'numpy.ndarray'> (3, 3) [[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [2. 2. 2.]]\n",
            "called value of t\n",
            "<class 'numpy.ndarray'> (3, 3) [[0. 1. 2.]\n",
            " [0. 1. 2.]\n",
            " [0. 1. 2.]]\n",
            "None\n",
            "shapes of p and t\n",
            "(2, 3)\n",
            "(2, 3)\n",
            "[[0.19332137 0.5        0.80667863]\n",
            " [0.19332137 0.5        0.80667863]]\n",
            "                                       function  loc  scale\n",
            "Healthcare employees                   logistic    1    0.7\n",
            "Non employees of healthcare companies  logistic    0    1.0\n",
            "[['logistic', 1, 0.7], ['logistic', 0, 1]]\n",
            "iterate over rows in dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-10bf95c33635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# note that in Dataframes, the rows are actually behind, so it is column major\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterate over rows in dataframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBehavioral_by_population_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'population'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBehavioral_by_population_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Week 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Behavioral_by_population_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhXkKFfuW1Gy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jggktC4lTNrs",
        "colab_type": "text"
      },
      "source": [
        "# Attachment: Test Code\n",
        "\n",
        "Used to test various features of the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1uq0RMITXy3",
        "colab_type": "text"
      },
      "source": [
        "## Test of cloning an external Repo\n",
        "\n",
        "NOte that this does a complete clone in the virtual machine, make sure you have enough space. Also you need to reclone when you close a Notebook instance, so this can be slow with lots of data.\n",
        "\n",
        "However, it does allow you checkout particular branches and have a realiable dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD9qp04tU9-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDnksm1MTmV7",
        "colab_type": "text"
      },
      "source": [
        "## Test of copying a single file from a repo\n",
        "\n",
        "This one way to get small datasets, you just point to the raw file and use `!curl` to bring it into the machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eqOYtkU99S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38KeztwBajb8",
        "colab_type": "text"
      },
      "source": [
        "## Test of connecting to Google Drive\n",
        "\n",
        "This we can use if we don't need a repo, but are just loading a static file. We normally want everything from a repo or reliable storage, but this is good for quick analysis. In most cases, you should just check this into a repo and then use the github raw extract instead so you get version control.\n",
        "\n",
        "Note that this does require an authentication everytime you start the Notebook, so the raw extract works better particularly if there it is a public repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKyKRJeoHtJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ldUhSATDRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXxIvzREazRq",
        "colab_type": "text"
      },
      "source": [
        "## Connecting two cells together for summaries with Cross-output Communications\n",
        "\n",
        "This is the best method for connecting the longer analysis to a cell that just has the executive summary data. _This does not appear to be working. Need to debug_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndtWkGTjlL70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%javascript\n",
        "const listenerChannel = new BroadcastChannel('channel');\n",
        "listenerChannel.onmessage = (msg) => {\n",
        "  const div = document.createElement('div');\n",
        "  div.textContent = msg.data;\n",
        "  document.body.appendChild(div);\n",
        "};"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmwMtzEslL7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%javascript\n",
        "const senderChannel = new BroadcastChannel('channel');\n",
        "senderChannel.postMessage('Hello world!');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AulXwNtb5yd",
        "colab_type": "text"
      },
      "source": [
        "## Creating forms for entry\n",
        "\n",
        "This is going to be used to parameterize models. This sets global variables that can be used in cells farther down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXWX8aG7lWvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Example form fields\n",
        "#@markdown Forms support many types of fields.\n",
        "\n",
        "no_type_checking = ''  #@param\n",
        "string_type = 'example'  #@param {type: \"string\"}\n",
        "slider_value = 142  #@param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  #@param {type: \"number\"}\n",
        "date = '2010-11-05'  #@param {type: \"date\"}\n",
        "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDR12BB4pPd1",
        "colab_type": "text"
      },
      "source": [
        "## Display Pandas data dataframes use Vega datasets as an example\n",
        "\n",
        "This uses the extension `google.colab.data_table` and there is a default data set called `vega_datasets` where you can extract data. It is not clear where the data is or how to figure out how ot use it. Google-fu does not help although the [source code](https://github.com/googlecolab/colabtools/blob/master/google/colab/data_table.py) tells us that `vega_dataset` has airport data in it.\n",
        "\n",
        "But the hing is in the name Vega which is a visualization package and [Vega Datasets access from Python](https://github.com/jakevdp/vega_datasets) are a standard set of data for visualization testing. The core datasets are kept in [github.io](https://vega.github.io/vega-datasets/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW43_ntJpdVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knd4KPzcpdUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vega_datasets import data\n",
        "data.cars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwkagE3vpdTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%unload_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKvLbYFyy8J-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgrbax2npdRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.stocks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8evpmsNgsOaF",
        "colab_type": "text"
      },
      "source": [
        "## Github Rendering of Jupyter\n",
        "\n",
        "This is pretty cool, but [Github](https://help.github.com/en/github/managing-files-in-a-repository/working-with-jupyter-notebook-files-on-github) actually renders the Jupyter notebooks as statis HTML when you browse it. That means just clicking on a `.ipynb` will give you something reasonable. It is not interactive nor is anything running behind it, but it does mean that documents produced by use are easily readable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RV8Y3kpyklo",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    }
  ]
}
